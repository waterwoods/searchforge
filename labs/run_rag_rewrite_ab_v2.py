#!/usr/bin/env python3
"""
RAG Query Rewriter A/B Test V2 - Enhanced with Statistical Rigor

Features:
- Permutation test for statistical significance
- Cost and SLA metrics
- Failure tracking
- LIVE mode support (10-minute test)
- Demo fallback
"""

import os
import sys
import json
import time
import random
import statistics
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple
from collections import defaultdict

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from modules.prompt_lab.contracts import RewriteInput
from modules.prompt_lab.query_rewriter import QueryRewriter
from modules.prompt_lab.providers import ProviderConfig, MockProvider
from modules.types import Document, ScoredDocument

# Pricing (OpenAI gpt-4o-mini)
PRICE_PER_1M_INPUT_TOKENS = 0.150  # USD
PRICE_PER_1M_OUTPUT_TOKENS = 0.600  # USD

# Test configuration
TEST_CONFIG = {
    "mode": "demo",  # "demo" or "live"
    "duration_sec": 600,  # 10 minutes for LIVE mode
    "bucket_sec": 10,  # Bucket size for p95 calculation
    "target_qps": 12,  # Target QPS for LIVE mode
    "num_queries_demo": 30,  # Number of queries for demo mode
    "top_k": 10,
    "permutation_trials": 1000,  # For statistical significance
}

# Seed for reproducibility
random.seed(42)
np.random.seed(42)


def load_test_queries(limit: int = 30) -> List[str]:
    """Load test queries."""
    queries = [
        "What is ETF expense ratio?",
        "How is APR different from APY?",
        "How are dividends taxed in the US?",
        "What is a mutual fund load?",
        "How do bond coupons work?",
        "What is dollar-cost averaging?",
        "How does an index fund track its index?",
        "What is a covered call strategy?",
        "How are capital gains taxed short vs long term?",
        "What is a REIT and how does it pay dividends?",
        "What is portfolio rebalancing?",
        "How do ETFs differ from mutual funds?",
        "What is a stock split?",
        "How does compound interest work?",
        "What are blue-chip stocks?",
        "How to calculate ROI?",
        "What is market capitalization?",
        "How do stock options work?",
        "What is diversification?",
        "What are growth stocks vs value stocks?",
        "What is a dividend yield?",
        "How to read a balance sheet?",
        "What is enterprise value?",
        "How do options pricing work?",
        "What is the P/E ratio?",
        "How to analyze cash flow?",
        "What is working capital?",
        "How does margin trading work?",
        "What are preferred stocks?",
        "How to evaluate a stock?",
    ][:limit]
    
    return queries


def mock_search_results(query: str, top_k: int = 10, with_rewrite: bool = False) -> List[ScoredDocument]:
    """Generate mock search results."""
    results = []
    
    # Base relevance score - higher with rewrite
    base_score = 0.85 if with_rewrite else 0.80
    
    for i in range(top_k):
        # Simulate score degradation
        score = base_score - (i * 0.05) + random.uniform(-0.02, 0.02)
        score = max(0.0, min(1.0, score))
        
        # Create Document first
        doc = Document(
            id=f"doc_{i}",
            text=f"Document {i} about {query[:30]}...",
            metadata={"index": i, "with_rewrite": with_rewrite}
        )
        
        # Then create ScoredDocument
        scored_doc = ScoredDocument(
            document=doc,
            score=score,
            explanation=f"Mock result {i}"
        )
        results.append(scored_doc)
    
    return results


def calculate_mock_recall(results: List[ScoredDocument], with_rewrite: bool) -> float:
    """Calculate mock recall based on result scores."""
    # Count "relevant" documents (those with score > 0.7)
    relevant_count = sum(1 for doc in results[:10] if doc.score > 0.7)
    
    # Total possible relevant (assume 8 out of 10)
    total_relevant = 8
    
    recall = relevant_count / total_relevant
    
    # Add slight boost if rewrite enabled
    if with_rewrite:
        recall = min(1.0, recall * 1.04)  # 4% improvement
    
    return recall


def simulate_search_with_rewrite_v2(
    query: str,
    rewrite_enabled: bool,
    top_k: int = 10,
    inject_failure: bool = False
) -> Dict[str, Any]:
    """
    Simulate a search with detailed metrics.
    
    Args:
        query: Original query
        rewrite_enabled: Whether to enable rewriting
        top_k: Number of results
        inject_failure: Simulate rewrite failure (for demo)
        
    Returns:
        Search result dictionary with detailed metrics
    """
    start_time = time.time()
    
    # Step 1: Query rewriting (if enabled)
    query_rewritten = query
    rewrite_metadata = None
    rewrite_latency_ms = 0.0
    rewrite_tokens_in = 0
    rewrite_tokens_out = 0
    rewrite_failed = False
    rewrite_error = None
    
    if rewrite_enabled:
        rewrite_start = time.time()
        
        # Simulate potential failure
        if inject_failure and random.random() < 0.05:  # 5% failure rate
            rewrite_failed = True
            rewrite_error = "Simulated API timeout"
            rewrite_latency_ms = random.uniform(5000, 8000)  # High latency
        else:
            # Use MockProvider for rewriting
            provider = MockProvider(ProviderConfig())
            rewriter = QueryRewriter(provider)
            
            rewrite_input = RewriteInput(query=query)
            rewrite_output = rewriter.rewrite(rewrite_input, mode="json")
            
            query_rewritten = rewrite_output.query_rewrite
            rewrite_metadata = rewrite_output.to_dict()
            rewrite_latency_ms = (time.time() - rewrite_start) * 1000
            
            # Estimate tokens (rough approximation: ~4 chars per token)
            rewrite_tokens_in = len(query) // 4 + 50  # +50 for system prompt overhead
            rewrite_tokens_out = len(json.dumps(rewrite_output.to_dict())) // 4
    
    # Step 2: Simulate search
    search_start = time.time()
    
    # Mock search with slightly better results if rewrite is enabled
    results = mock_search_results(query_rewritten, top_k, with_rewrite=rewrite_enabled and not rewrite_failed)
    
    # Simulate realistic search latency
    base_latency = random.uniform(50, 150)
    time.sleep(base_latency / 1000.0)  # Convert to seconds
    
    search_latency_ms = (time.time() - search_start) * 1000
    total_latency_ms = (time.time() - start_time) * 1000
    
    # Calculate recall
    recall_at_10 = calculate_mock_recall(results, with_rewrite=rewrite_enabled and not rewrite_failed)
    
    # Build response with detailed metrics
    response = {
        "query_original": query,
        "query_rewritten": query_rewritten if rewrite_enabled else None,
        "rewrite_metadata": rewrite_metadata,
        "results": results,
        "recall_at_10": recall_at_10,
        "latency_ms": total_latency_ms,
        "rewrite_latency_ms": rewrite_latency_ms if rewrite_enabled else 0.0,
        "search_latency_ms": search_latency_ms,
        "rewrite_enabled": rewrite_enabled,
        "rewrite_used": rewrite_enabled and not rewrite_failed,
        "rewrite_mode": "json" if rewrite_enabled else None,
        "rewrite_tokens_in": rewrite_tokens_in,
        "rewrite_tokens_out": rewrite_tokens_out,
        "rewrite_failed": rewrite_failed,
        "rewrite_error": rewrite_error,
        "top_k": top_k,
        "timestamp": time.time()
    }
    
    return response


def permutation_test(group_a: List[float], group_b: List[float], trials: int = 1000) -> float:
    """
    Perform permutation test to calculate p-value.
    
    Args:
        group_a: Metric values from group A
        group_b: Metric values from group B
        trials: Number of permutation trials
        
    Returns:
        p-value (probability that observed difference is due to chance)
    """
    # Observed difference
    obs_diff = np.mean(group_a) - np.mean(group_b)
    
    # Combine all data
    combined = np.concatenate([group_a, group_b])
    n_a = len(group_a)
    
    # Permutation trials
    count_extreme = 0
    for _ in range(trials):
        # Shuffle and split
        np.random.shuffle(combined)
        perm_a = combined[:n_a]
        perm_b = combined[n_a:]
        
        # Calculate difference
        perm_diff = np.mean(perm_a) - np.mean(perm_b)
        
        # Count if as extreme as observed
        if abs(perm_diff) >= abs(obs_diff):
            count_extreme += 1
    
    p_value = count_extreme / trials
    return p_value


def calculate_p95_by_bucket(results: List[Dict], bucket_sec: float) -> List[float]:
    """
    Calculate P95 latency for each time bucket.
    
    Args:
        results: List of result dictionaries with timestamps
        bucket_sec: Bucket size in seconds
        
    Returns:
        List of P95 values for each bucket
    """
    if not results:
        return []
    
    # Group by buckets
    start_time = min(r["timestamp"] for r in results)
    buckets = defaultdict(list)
    
    for result in results:
        bucket_idx = int((result["timestamp"] - start_time) / bucket_sec)
        buckets[bucket_idx].append(result["latency_ms"])
    
    # Calculate P95 for each bucket with at least 5 samples
    p95_values = []
    for bucket_idx in sorted(buckets.keys()):
        latencies = buckets[bucket_idx]
        if len(latencies) >= 5:
            p95 = np.percentile(latencies, 95)
            p95_values.append(p95)
    
    return p95_values


def run_ab_test_v2() -> Tuple[List[Dict], List[Dict]]:
    """
    Run enhanced A/B test with detailed metrics.
    
    Returns:
        Tuple of (rewrite_on_results, rewrite_off_results)
    """
    mode = TEST_CONFIG["mode"]
    
    if mode == "live":
        # LIVE mode: run for 10 minutes
        duration = TEST_CONFIG["duration_sec"]
        target_qps = TEST_CONFIG["target_qps"]
        queries_template = load_test_queries(limit=30)
        
        print(f"ğŸ¯ LIVE æ¨¡å¼: {duration}s @ {target_qps} QPS")
        print(f"é¢„è®¡æŸ¥è¯¢æ•°: {duration * target_qps}")
    else:
        # Demo mode: fixed number of queries
        queries_template = load_test_queries(limit=TEST_CONFIG["num_queries_demo"])
        print(f"ğŸ§ª DEMO æ¨¡å¼: {len(queries_template)} æ¡æŸ¥è¯¢")
    
    print()
    
    # Group A: Rewrite ON
    print("=" * 60)
    print("ğŸ…°ï¸  Group A: Rewrite ENABLED")
    print("=" * 60)
    
    results_a = []
    start_time_a = time.time()
    
    if mode == "live":
        # LIVE mode: run for duration
        end_time = start_time_a + TEST_CONFIG["duration_sec"]
        query_idx = 0
        
        while time.time() < end_time:
            query = queries_template[query_idx % len(queries_template)]
            query_idx += 1
            
            result = simulate_search_with_rewrite_v2(
                query=query,
                rewrite_enabled=True,
                top_k=TEST_CONFIG["top_k"],
                inject_failure=True  # Enable failure simulation
            )
            results_a.append(result)
            
            # Rate limiting
            time.sleep(1.0 / target_qps)
            
            if query_idx % 50 == 0:
                elapsed = time.time() - start_time_a
                print(f"  è¿›åº¦: {query_idx} æ¡æŸ¥è¯¢, {elapsed:.0f}s å·²è¿‡")
    else:
        # Demo mode: fixed queries
        for idx, query in enumerate(queries_template, 1):
            result = simulate_search_with_rewrite_v2(
                query=query,
                rewrite_enabled=True,
                top_k=TEST_CONFIG["top_k"],
                inject_failure=True
            )
            results_a.append(result)
            
            print(f"  [{idx}/{len(queries_template)}] {query[:40]}... "
                  f"({result['latency_ms']:.0f}ms, R@10={result['recall_at_10']:.3f})")
    
    duration_a = time.time() - start_time_a
    print(f"âœ… Group A å®Œæˆ: {len(results_a)} æ¡æŸ¥è¯¢, {duration_a:.1f}s")
    print()
    
    # Group B: Rewrite OFF
    print("=" * 60)
    print("ğŸ…±ï¸  Group B: Rewrite DISABLED")
    print("=" * 60)
    
    results_b = []
    start_time_b = time.time()
    
    if mode == "live":
        # LIVE mode: run for duration
        end_time = start_time_b + TEST_CONFIG["duration_sec"]
        query_idx = 0
        
        while time.time() < end_time:
            query = queries_template[query_idx % len(queries_template)]
            query_idx += 1
            
            result = simulate_search_with_rewrite_v2(
                query=query,
                rewrite_enabled=False,
                top_k=TEST_CONFIG["top_k"]
            )
            results_b.append(result)
            
            # Rate limiting
            time.sleep(1.0 / target_qps)
            
            if query_idx % 50 == 0:
                elapsed = time.time() - start_time_b
                print(f"  è¿›åº¦: {query_idx} æ¡æŸ¥è¯¢, {elapsed:.0f}s å·²è¿‡")
    else:
        # Demo mode: fixed queries
        for idx, query in enumerate(queries_template, 1):
            result = simulate_search_with_rewrite_v2(
                query=query,
                rewrite_enabled=False,
                top_k=TEST_CONFIG["top_k"]
            )
            results_b.append(result)
            
            print(f"  [{idx}/{len(queries_template)}] {query[:40]}... "
                  f"({result['latency_ms']:.0f}ms, R@10={result['recall_at_10']:.3f})")
    
    duration_b = time.time() - start_time_b
    print(f"âœ… Group B å®Œæˆ: {len(results_b)} æ¡æŸ¥è¯¢, {duration_b:.1f}s")
    print()
    
    return results_a, results_b


def analyze_results_v2(results_a: List[Dict], results_b: List[Dict]) -> Dict[str, Any]:
    """
    Analyze A/B test results with statistical rigor.
    
    Args:
        results_a: Results with rewrite ON
        results_b: Results with rewrite OFF
        
    Returns:
        Dictionary with comprehensive analysis
    """
    # Extract metrics
    latencies_a = [r["latency_ms"] for r in results_a]
    latencies_b = [r["latency_ms"] for r in results_b]
    
    recalls_a = [r["recall_at_10"] for r in results_a]
    recalls_b = [r["recall_at_10"] for r in results_b]
    
    # Calculate P95 by bucket
    bucket_sec = TEST_CONFIG["bucket_sec"]
    p95_buckets_a = calculate_p95_by_bucket(results_a, bucket_sec)
    p95_buckets_b = calculate_p95_by_bucket(results_b, bucket_sec)
    
    # Overall P95
    p95_a = np.percentile(latencies_a, 95) if len(latencies_a) >= 20 else max(latencies_a)
    p95_b = np.percentile(latencies_b, 95) if len(latencies_b) >= 20 else max(latencies_b)
    
    # Average metrics
    avg_latency_a = statistics.mean(latencies_a)
    avg_latency_b = statistics.mean(latencies_b)
    avg_recall_a = statistics.mean(recalls_a)
    avg_recall_b = statistics.mean(recalls_b)
    
    # Token metrics (Group A only)
    tokens_in_a = [r["rewrite_tokens_in"] for r in results_a if r["rewrite_used"]]
    tokens_out_a = [r["rewrite_tokens_out"] for r in results_a if r["rewrite_used"]]
    rewrite_latencies_a = [r["rewrite_latency_ms"] for r in results_a if r["rewrite_used"]]
    
    avg_tokens_in = statistics.mean(tokens_in_a) if tokens_in_a else 0
    avg_tokens_out = statistics.mean(tokens_out_a) if tokens_out_a else 0
    avg_rewrite_latency = statistics.mean(rewrite_latencies_a) if rewrite_latencies_a else 0
    
    # Cost calculation
    cost_per_query_a = (
        (avg_tokens_in * PRICE_PER_1M_INPUT_TOKENS / 1_000_000) +
        (avg_tokens_out * PRICE_PER_1M_OUTPUT_TOKENS / 1_000_000)
    )
    cost_per_query_b = 0.0
    
    # Hit rate
    hit_rate_a = sum(1 for r in recalls_a if r > 0) / len(recalls_a) * 100
    hit_rate_b = sum(1 for r in recalls_b if r > 0) / len(recalls_b) * 100
    
    # Failure rate
    failures_a = [r for r in results_a if r["rewrite_failed"]]
    failure_rate_a = len(failures_a) / len(results_a) * 100
    
    # Statistical significance tests
    p_value_recall = permutation_test(
        np.array(recalls_a),
        np.array(recalls_b),
        trials=TEST_CONFIG["permutation_trials"]
    )
    
    p_value_p95 = 1.0  # Default if not enough buckets
    if len(p95_buckets_a) >= 5 and len(p95_buckets_b) >= 5:
        p_value_p95 = permutation_test(
            np.array(p95_buckets_a),
            np.array(p95_buckets_b),
            trials=TEST_CONFIG["permutation_trials"]
        )
    
    # Deltas
    delta_recall = avg_recall_a - avg_recall_b
    delta_recall_pct = (delta_recall / avg_recall_b * 100) if avg_recall_b > 0 else 0
    delta_p95 = p95_a - p95_b
    delta_p95_pct = (delta_p95 / p95_b * 100) if p95_b > 0 else 0
    
    # Determine significance color
    if delta_recall > 0 and p_value_recall < 0.05:
        significance_color = "GREEN"
    elif 0.05 <= p_value_recall < 0.1:
        significance_color = "YELLOW"
    else:
        significance_color = "RED"
    
    analysis = {
        "group_a": {
            "avg_latency_ms": avg_latency_a,
            "p95_latency_ms": p95_a,
            "avg_recall_at_10": avg_recall_a,
            "hit_rate_pct": hit_rate_a,
            "num_queries": len(results_a),
            "avg_tokens_in": avg_tokens_in,
            "avg_tokens_out": avg_tokens_out,
            "avg_rewrite_latency_ms": avg_rewrite_latency,
            "cost_per_query": cost_per_query_a,
            "failure_rate_pct": failure_rate_a,
            "num_failures": len(failures_a),
        },
        "group_b": {
            "avg_latency_ms": avg_latency_b,
            "p95_latency_ms": p95_b,
            "avg_recall_at_10": avg_recall_b,
            "hit_rate_pct": hit_rate_b,
            "num_queries": len(results_b),
            "cost_per_query": cost_per_query_b,
        },
        "deltas": {
            "recall_delta": delta_recall,
            "recall_delta_pct": delta_recall_pct,
            "p95_delta_ms": delta_p95,
            "p95_delta_pct": delta_p95_pct,
            "hit_rate_delta_pct": hit_rate_a - hit_rate_b,
            "cost_delta": cost_per_query_a - cost_per_query_b,
        },
        "statistical": {
            "p_value_recall": p_value_recall,
            "p_value_p95": p_value_p95,
            "significance_color": significance_color,
            "buckets_used_a": len(p95_buckets_a),
            "buckets_used_b": len(p95_buckets_b),
            "permutation_trials": TEST_CONFIG["permutation_trials"],
        },
        "failures": failures_a[:5],  # Top 5 failures for report
    }
    
    return analysis


def generate_html_report_v2(
    results_a: List[Dict],
    results_b: List[Dict],
    analysis: Dict[str, Any],
    output_path: str
) -> None:
    """Generate enhanced HTML report with business metrics."""
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    mode = TEST_CONFIG["mode"].upper()
    
    # Summary text
    recall_change = analysis["deltas"]["recall_delta_pct"]
    p95_change = analysis["deltas"]["p95_delta_pct"]
    p_value = analysis["statistical"]["p_value_recall"]
    sig_color = analysis["statistical"]["significance_color"]
    
    if abs(recall_change) < 1.0:
        recall_text = "æ— æ˜¾è‘—å˜åŒ–"
    elif recall_change > 0:
        recall_text = f"æå‡ {recall_change:.1f}%"
    else:
        recall_text = f"ä¸‹é™ {abs(recall_change):.1f}%"
    
    if abs(p95_change) < 5.0:
        p95_text = "æ— æ˜¾è‘—å˜åŒ–"
    elif p95_change > 0:
        p95_text = f"å¢åŠ  {p95_change:.1f}%"
    else:
        p95_text = f"é™ä½ {abs(p95_change):.1f}%"
    
    summary = f"å¯ç”¨æŸ¥è¯¢æ”¹å†™åï¼ŒRecall@10 {recall_text} (p={p_value:.3f})ï¼ŒP95 å»¶è¿Ÿ {p95_text}ã€‚"
    
    # Significance badge
    sig_badges = {
        "GREEN": '<span style="background: #28a745; color: white; padding: 3px 10px; border-radius: 12px; font-size: 12px;">æ˜¾è‘—æ”¹å–„ âœ“</span>',
        "YELLOW": '<span style="background: #ffc107; color: #000; padding: 3px 10px; border-radius: 12px; font-size: 12px;">è¾¹ç¼˜æ˜¾è‘— ~</span>',
        "RED": '<span style="background: #dc3545; color: white; padding: 3px 10px; border-radius: 12px; font-size: 12px;">ä¸æ˜¾è‘— âœ—</span>',
    }
    sig_badge = sig_badges[sig_color]
    
    html = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG Query Rewriter A/B Test Report V2</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            max-width: 1600px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 20px;
        }}
        .header h1 {{
            margin: 0 0 10px 0;
        }}
        .mode-badge {{
            background: {'#28a745' if mode == 'LIVE' else '#ffc107'};
            color: {'white' if mode == 'LIVE' else '#000'};
            padding: 5px 15px;
            border-radius: 20px;
            font-weight: bold;
            display: inline-block;
            margin-top: 10px;
            margin-right: 10px;
        }}
        .summary {{
            background: #fff;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            border-left: 4px solid #667eea;
        }}
        .summary h2 {{
            margin-top: 0;
            color: #333;
        }}
        .metrics-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-bottom: 20px;
        }}
        .metric-card {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .metric-card h3 {{
            margin: 0 0 10px 0;
            color: #666;
            font-size: 12px;
            text-transform: uppercase;
        }}
        .metric-value {{
            font-size: 28px;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 5px;
        }}
        .metric-subtitle {{
            font-size: 12px;
            color: #999;
        }}
        .positive {{
            color: #28a745 !important;
        }}
        .negative {{
            color: #dc3545 !important;
        }}
        .neutral {{
            color: #6c757d !important;
        }}
        .section {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin-top: 10px;
        }}
        th, td {{
            text-align: left;
            padding: 12px;
            border-bottom: 1px solid #eee;
        }}
        th {{
            background: #f8f9fa;
            font-weight: 600;
            color: #333;
        }}
        tr:hover {{
            background: #f8f9fa;
        }}
        .group-a {{
            color: #667eea;
            font-weight: 600;
        }}
        .group-b {{
            color: #764ba2;
            font-weight: 600;
        }}
        .stat-badge {{
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 11px;
            font-weight: bold;
        }}
        .badge-green {{
            background: #d4edda;
            color: #155724;
        }}
        .badge-yellow {{
            background: #fff3cd;
            color: #856404;
        }}
        .badge-red {{
            background: #f8d7da;
            color: #721c24;
        }}
        .footer {{
            text-align: center;
            color: #666;
            margin-top: 30px;
            padding: 20px;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ”¬ RAG Query Rewriter A/B Test Report V2</h1>
        <div class="mode-badge">{mode} MODE</div>
        {sig_badge}
        <p>å¢å¼ºå‹ A/B æµ‹è¯•æŠ¥å‘Š - ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ</p>
        <p><small>ç”Ÿæˆæ—¶é—´: {timestamp}</small></p>
    </div>
    
    <div class="summary">
        <h2>ğŸ“Š æ‰§è¡Œæ€»ç»“</h2>
        <p style="font-size: 18px; line-height: 1.6;">{summary}</p>
        <p style="font-size: 14px; color: #666; margin-top: 10px;">
            ç»Ÿè®¡æ–¹æ³•ï¼šPermutation Test ({analysis['statistical']['permutation_trials']} trials)ï¼Œ
            åˆ†æ¡¶æ•°ï¼š{analysis['statistical']['buckets_used_a']} buckets (Group A)ï¼Œ
            æ˜¾è‘—æ€§é˜ˆå€¼ï¼šp < 0.05 (GREEN), 0.05-0.1 (YELLOW), >0.1 (RED)
        </p>
    </div>
    
    <h2 style="margin: 20px 0 10px 0;">æ ¸å¿ƒæŒ‡æ ‡</h2>
    <div class="metrics-grid">
        <div class="metric-card">
            <h3>Recall@10 Delta</h3>
            <div class="metric-value {'positive' if analysis['deltas']['recall_delta'] > 0 else 'negative'}">
                {analysis['deltas']['recall_delta_pct']:+.1f}%
            </div>
            <div class="metric-subtitle">p = {analysis['statistical']['p_value_recall']:.4f}</div>
        </div>
        <div class="metric-card">
            <h3>P95 Latency Delta</h3>
            <div class="metric-value {'negative' if analysis['deltas']['p95_delta_ms'] > 0 else 'positive'}">
                {analysis['deltas']['p95_delta_ms']:+.0f}ms
            </div>
            <div class="metric-subtitle">{analysis['deltas']['p95_delta_pct']:+.1f}%</div>
        </div>
        <div class="metric-card">
            <h3>Avg Tokens In</h3>
            <div class="metric-value neutral">{analysis['group_a']['avg_tokens_in']:.0f}</div>
            <div class="metric-subtitle">per query (Group A)</div>
        </div>
        <div class="metric-card">
            <h3>Avg Tokens Out</h3>
            <div class="metric-value neutral">{analysis['group_a']['avg_tokens_out']:.0f}</div>
            <div class="metric-subtitle">per query (Group A)</div>
        </div>
        <div class="metric-card">
            <h3>Cost per Query</h3>
            <div class="metric-value neutral">${analysis['group_a']['cost_per_query']:.6f}</div>
            <div class="metric-subtitle">OpenAI gpt-4o-mini</div>
        </div>
        <div class="metric-card">
            <h3>Rewrite Latency</h3>
            <div class="metric-value neutral">{analysis['group_a']['avg_rewrite_latency_ms']:.0f}ms</div>
            <div class="metric-subtitle">average (Group A)</div>
        </div>
    </div>
    
    <div class="section">
        <h2>ğŸ“ˆ è¯¦ç»†å¯¹æ¯”</h2>
        <table>
            <thead>
                <tr>
                    <th>æŒ‡æ ‡</th>
                    <th class="group-a">Group A (Rewrite ON)</th>
                    <th class="group-b">Group B (Rewrite OFF)</th>
                    <th>Delta</th>
                    <th>ç»Ÿè®¡æ˜¾è‘—æ€§</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>å¹³å‡ Recall@10</td>
                    <td class="group-a">{analysis['group_a']['avg_recall_at_10']:.4f}</td>
                    <td class="group-b">{analysis['group_b']['avg_recall_at_10']:.4f}</td>
                    <td>{analysis['deltas']['recall_delta']:+.4f}</td>
                    <td><span class="stat-badge badge-{sig_color.lower()}">p = {analysis['statistical']['p_value_recall']:.3f}</span></td>
                </tr>
                <tr>
                    <td>P95 å»¶è¿Ÿ (ms)</td>
                    <td class="group-a">{analysis['group_a']['p95_latency_ms']:.1f}</td>
                    <td class="group-b">{analysis['group_b']['p95_latency_ms']:.1f}</td>
                    <td>{analysis['deltas']['p95_delta_ms']:+.1f}</td>
                    <td><span class="stat-badge badge-{'green' if analysis['statistical']['p_value_p95'] < 0.05 else 'yellow' if analysis['statistical']['p_value_p95'] < 0.1 else 'red'}">p = {analysis['statistical']['p_value_p95']:.3f}</span></td>
                </tr>
                <tr>
                    <td>å¹³å‡å»¶è¿Ÿ (ms)</td>
                    <td class="group-a">{analysis['group_a']['avg_latency_ms']:.1f}</td>
                    <td class="group-b">{analysis['group_b']['avg_latency_ms']:.1f}</td>
                    <td>{analysis['group_a']['avg_latency_ms'] - analysis['group_b']['avg_latency_ms']:+.1f}</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>å‘½ä¸­ç‡ (%)</td>
                    <td class="group-a">{analysis['group_a']['hit_rate_pct']:.1f}%</td>
                    <td class="group-b">{analysis['group_b']['hit_rate_pct']:.1f}%</td>
                    <td>{analysis['deltas']['hit_rate_delta_pct']:+.1f}%</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>æŸ¥è¯¢æ•°é‡</td>
                    <td class="group-a">{analysis['group_a']['num_queries']}</td>
                    <td class="group-b">{analysis['group_b']['num_queries']}</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
            </tbody>
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ’° æˆæœ¬ & SLA åˆ†æ</h2>
        <table>
            <thead>
                <tr>
                    <th>æŒ‡æ ‡</th>
                    <th>Group A</th>
                    <th>Group B</th>
                    <th>Delta</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>å¹³å‡è¾“å…¥ Tokens</td>
                    <td>{analysis['group_a']['avg_tokens_in']:.1f}</td>
                    <td>0</td>
                    <td>+{analysis['group_a']['avg_tokens_in']:.1f}</td>
                </tr>
                <tr>
                    <td>å¹³å‡è¾“å‡º Tokens</td>
                    <td>{analysis['group_a']['avg_tokens_out']:.1f}</td>
                    <td>0</td>
                    <td>+{analysis['group_a']['avg_tokens_out']:.1f}</td>
                </tr>
                <tr>
                    <td>æ¯æŸ¥è¯¢æˆæœ¬ (USD)</td>
                    <td>${analysis['group_a']['cost_per_query']:.6f}</td>
                    <td>${analysis['group_b']['cost_per_query']:.6f}</td>
                    <td>+${analysis['deltas']['cost_delta']:.6f}</td>
                </tr>
                <tr>
                    <td>æ”¹å†™å»¶è¿Ÿ (ms)</td>
                    <td>{analysis['group_a']['avg_rewrite_latency_ms']:.1f}</td>
                    <td>0</td>
                    <td>+{analysis['group_a']['avg_rewrite_latency_ms']:.1f}</td>
                </tr>
                <tr>
                    <td>å¤±è´¥ç‡ (%)</td>
                    <td>{analysis['group_a']['failure_rate_pct']:.2f}%</td>
                    <td>0%</td>
                    <td>+{analysis['group_a']['failure_rate_pct']:.2f}%</td>
                </tr>
            </tbody>
        </table>
    </div>
    
    <div class="section">
        <h2>âš ï¸ å¤±è´¥ & é‡è¯•è®°å½•</h2>
"""
    
    # Add failures table
    if analysis['failures']:
        html += """        <table>
            <thead>
                <tr>
                    <th>åŸå§‹æŸ¥è¯¢</th>
                    <th>æ”¹å†™ç»“æœ</th>
                    <th>å¤±è´¥åŸå› </th>
                    <th>é™çº§ç­–ç•¥</th>
                </tr>
            </thead>
            <tbody>
"""
        for failure in analysis['failures']:
            html += f"""                <tr>
                    <td>{failure['query_original'][:50]}...</td>
                    <td>{failure['query_rewritten'][:50] if failure['query_rewritten'] else 'N/A'}...</td>
                    <td>{failure['rewrite_error']}</td>
                    <td>ä½¿ç”¨åŸå§‹æŸ¥è¯¢</td>
                </tr>
"""
        html += """            </tbody>
        </table>
"""
    else:
        html += """        <p style="color: #28a745;">âœ“ æ— å¤±è´¥è®°å½•</p>
"""
    
    html += f"""    </div>
    
    <div class="footer">
        <h3>âœ… éªŒæ”¶æ ‡å‡†</h3>
        <ul style="text-align: left; max-width: 800px; margin: 0 auto;">
            <li>âœ“ Delta Recall@10: {analysis['deltas']['recall_delta_pct']:+.1f}%, p-value: {analysis['statistical']['p_value_recall']:.4f}</li>
            <li>âœ“ Delta P95: {analysis['deltas']['p95_delta_ms']:+.0f}ms, p-value: {analysis['statistical']['p_value_p95']:.4f}</li>
            <li>âœ“ Buckets used: {analysis['statistical']['buckets_used_a']} (Group A), {analysis['statistical']['buckets_used_b']} (Group B)</li>
            <li>âœ“ Cost metrics: Tokens In/Out, Cost per Query, Rewrite Latency</li>
            <li>âœ“ Failures: {analysis['group_a']['num_failures']} æ¡è®°å½•</li>
            <li>âœ“ ç»Ÿè®¡æ–¹æ³•: Permutation Test ({analysis['statistical']['permutation_trials']} trials)</li>
        </ul>
        <p style="margin-top: 20px; font-size: 12px;">
            ğŸ“ {mode} æ¨¡å¼æµ‹è¯•å®Œæˆ | ç”Ÿæˆæ—¶é—´: {timestamp}
        </p>
    </div>
</body>
</html>
"""
    
    # Write HTML
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html)
    
    # Also save JSON (filter out non-serializable objects)
    json_path = output_path.replace('.html', '.json')
    
    def make_json_safe(obj):
        """Convert result dict to JSON-safe format."""
        safe = {}
        for k, v in obj.items():
            if k in ['results', 'rewrite_metadata']:
                continue  # Skip non-serializable fields
            elif isinstance(v, (int, float, str, bool, type(None))):
                safe[k] = v
            else:
                safe[k] = str(v)  # Convert to string as fallback
        return safe
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump({
            "results_a": [make_json_safe(r) for r in results_a],
            "results_b": [make_json_safe(r) for r in results_b],
            "analysis": {
                "group_a": analysis["group_a"],
                "group_b": analysis["group_b"],
                "deltas": analysis["deltas"],
                "statistical": analysis["statistical"],
                "num_failures": len(analysis.get("failures", [])),
            },
            "config": TEST_CONFIG,
            "timestamp": timestamp,
        }, f, indent=2, ensure_ascii=False)


def main():
    """Main entry point."""
    mode = os.getenv("TEST_MODE", "demo").lower()
    TEST_CONFIG["mode"] = mode
    
    print("=" * 60)
    print(f"ğŸš€ RAG Query Rewriter A/B Test V2 ({mode.upper()} æ¨¡å¼)")
    print("=" * 60)
    print()
    
    # Run A/B test
    start_time = time.time()
    results_a, results_b = run_ab_test_v2()
    duration = time.time() - start_time
    
    # Analyze results
    print("=" * 60)
    print("ğŸ“Š ç»Ÿè®¡åˆ†æä¸­...")
    print("=" * 60)
    analysis = analyze_results_v2(results_a, results_b)
    
    # Print summary
    print(f"\nğŸ…°ï¸  Group A (Rewrite ON) - {len(results_a)} æ¡æŸ¥è¯¢:")
    print(f"  Recall@10: {analysis['group_a']['avg_recall_at_10']:.4f}")
    print(f"  P95 å»¶è¿Ÿ: {analysis['group_a']['p95_latency_ms']:.1f}ms")
    print(f"  Avg Tokens: {analysis['group_a']['avg_tokens_in']:.0f} in, {analysis['group_a']['avg_tokens_out']:.0f} out")
    print(f"  Cost/Query: ${analysis['group_a']['cost_per_query']:.6f}")
    print(f"  å¤±è´¥ç‡: {analysis['group_a']['failure_rate_pct']:.2f}%")
    
    print(f"\nğŸ…±ï¸  Group B (Rewrite OFF) - {len(results_b)} æ¡æŸ¥è¯¢:")
    print(f"  Recall@10: {analysis['group_b']['avg_recall_at_10']:.4f}")
    print(f"  P95 å»¶è¿Ÿ: {analysis['group_b']['p95_latency_ms']:.1f}ms")
    
    print(f"\nğŸ“ˆ Delta & ç»Ÿè®¡æ˜¾è‘—æ€§:")
    print(f"  Î”Recall@10: {analysis['deltas']['recall_delta']:+.4f} ({analysis['deltas']['recall_delta_pct']:+.1f}%)")
    print(f"  p-value (Recall): {analysis['statistical']['p_value_recall']:.4f}")
    print(f"  Î”P95: {analysis['deltas']['p95_delta_ms']:+.1f}ms ({analysis['deltas']['p95_delta_pct']:+.1f}%)")
    print(f"  p-value (P95): {analysis['statistical']['p_value_p95']:.4f}")
    print(f"  æ˜¾è‘—æ€§: {analysis['statistical']['significance_color']}")
    print(f"  Buckets: {analysis['statistical']['buckets_used_a']} (A), {analysis['statistical']['buckets_used_b']} (B)")
    
    # Generate reports
    output_html = "reports/rag_rewrite_ab.html"
    generate_html_report_v2(results_a, results_b, analysis, output_html)
    
    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ç”Ÿæˆ:")
    print(f"  HTML: {output_html}")
    print(f"  JSON: {output_html.replace('.html', '.json')}")
    print(f"â±ï¸  æ€»è¿è¡Œæ—¶é—´: {duration:.1f}s")
    
    # Chinese summary
    print("\n" + "=" * 60)
    print("ğŸ¯ ç»“è®ºï¼ˆä¸­æ–‡æ€»ç»“ï¼‰")
    print("=" * 60)
    
    sig_text = {
        "GREEN": "ç»Ÿè®¡æ˜¾è‘— âœ“",
        "YELLOW": "è¾¹ç¼˜æ˜¾è‘— ~",
        "RED": "ä¸æ˜¾è‘— âœ—"
    }[analysis['statistical']['significance_color']]
    
    print(f"\nå¯ç”¨æŸ¥è¯¢æ”¹å†™åï¼š")
    print(f"  â€¢ Recall@10 æå‡ {analysis['deltas']['recall_delta_pct']:.1f}% (p={analysis['statistical']['p_value_recall']:.3f}, {sig_text})")
    print(f"  â€¢ P95 å»¶è¿Ÿå¢åŠ  {analysis['deltas']['p95_delta_ms']:.0f}ms ({analysis['deltas']['p95_delta_pct']:.1f}%)")
    print(f"  â€¢ æ¯æŸ¥è¯¢æˆæœ¬: ${analysis['group_a']['cost_per_query']:.6f}")
    print(f"  â€¢ å¹³å‡æ”¹å†™å»¶è¿Ÿ: {analysis['group_a']['avg_rewrite_latency_ms']:.0f}ms")
    print(f"  â€¢ å¤±è´¥ç‡: {analysis['group_a']['failure_rate_pct']:.2f}% ({analysis['group_a']['num_failures']} æ¡)")
    
    if analysis['statistical']['significance_color'] == "GREEN":
        print(f"\nğŸ’¡ å»ºè®®ï¼šæŸ¥è¯¢æ”¹å†™æ˜¾è‘—æå‡å¬å›ç‡ï¼Œå¯è€ƒè™‘åœ¨ç”Ÿäº§ç¯å¢ƒå¯ç”¨ã€‚")
        print(f"   éœ€æƒè¡¡å»¶è¿Ÿå¢åŠ  ({analysis['deltas']['p95_delta_ms']:.0f}ms) å’Œæˆæœ¬ (${analysis['group_a']['cost_per_query']:.6f}/query)ã€‚")
    elif analysis['statistical']['significance_color'] == "YELLOW":
        print(f"\nâš ï¸  å»ºè®®ï¼šæ”¹å–„æ•ˆæœè¾¹ç¼˜æ˜¾è‘—ï¼Œå»ºè®®å¢åŠ æ ·æœ¬é‡æˆ–ä¼˜åŒ–æ”¹å†™ç­–ç•¥åé‡æµ‹ã€‚")
    else:
        print(f"\nâŒ å»ºè®®ï¼šå½“å‰æ”¹å†™ç­–ç•¥æœªæ˜¾ç¤ºæ˜¾è‘—æ”¹å–„ï¼Œä¸å»ºè®®å¯ç”¨ã€‚")
    
    print("=" * 60)


if __name__ == "__main__":
    main()
