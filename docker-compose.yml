services:
  base:
    build:
      context: .
      dockerfile: docker/base/Dockerfile
    image: searchforge-base:py311
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    ports: [ "6379:6379" ]
    command: redis-server --appendonly yes
    volumes: [ "redis_data:/data" ]
  qdrant:
    image: qdrant/qdrant:${QDRANT_TAG:-v1.8.4}
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./.qdrant:/qdrant/storage
  retrieval-proxy:
    build:
      context: .
      dockerfile: services/retrieval_proxy/Dockerfile
    env_file:
      - .env.current
    environment:
      - PORT=7070
      - QDRANT_URL=http://qdrant:6333
      # TIMEOUT_MS=400
      # BUDGET_MS=400
      # RRF_K=60
      # TOPK_INIT=24
      # TOPK_MAX=64
      # CACHE_TTL=60
    depends_on:
      - qdrant
    ports: [ "7070:7070" ]
    restart: unless-stopped
  milvus-etcd:
    image: quay.io/coreos/etcd:v3.5.5
    restart: unless-stopped
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - milvus_etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
  milvus-minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    restart: unless-stopped
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - milvus_minio:/minio_data
    command: minio server /minio_data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3
  milvus-standalone:
    image: milvusdb/milvus:v2.3.3
    restart: unless-stopped
    command: [ "milvus", "run", "standalone" ]
    environment:
      ETCD_ENDPOINTS: milvus-etcd:2379
      MINIO_ADDRESS: milvus-minio:9000
    volumes:
      - milvus_data:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - milvus-etcd
      - milvus-minio
  rag-api:
    build:
      context: .
      dockerfile: services/fiqa_api/Dockerfile
      args:
        MAIN_PORT: ${MAIN_PORT:-8000}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
    env_file:
      - .env.current
    environment:
      - MAIN_PORT=8000
      - UVICORN_WORKERS=1
      - OMP_NUM_THREADS=1
      - OPENAI_TIMEOUT_MS=30000
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - MILVUS_HOST=milvus-standalone
      - USE_LANGGRAPH_SINGLE_HOME=true
      - MILVUS_PORT=19530
      - VECTOR_BACKEND=${VECTOR_BACKEND:-faiss}
      - SENTENCE_TRANSFORMERS_HOME=/app/models
      - HF_HOME=/app/models
      - TRANSFORMERS_CACHE=/app/models
      - RAG_API_SLA_SAMPLING=${RAG_API_SLA_SAMPLING:-on}
      - SLA_WINDOW_SEC=${SLA_WINDOW_SEC:-120}
      - EXPERIMENTS_ROOT=/app/experiments
      - MPLBACKEND=Agg
      # Ensure /app is in PYTHONPATH for module imports
      - PYTHONPATH=/app:${PYTHONPATH:-}
      # CPU-only SBERT embedding configuration
      - EMBEDDING_BACKEND=SBERT
      - SBERT_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - EXPECTED_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - FAST_STARTUP=1
      - INIT_TIMEOUT_SEC=30
      - QUERY_TIMEOUT_S=${QUERY_TIMEOUT_S:-45}
      - OBS_ENABLED=${OBS_ENABLED:-0}
      - OBS_SAMPLE_RATE=${OBS_SAMPLE_RATE:-0.2}
      - OBS_TIMEOUT_MS=${OBS_TIMEOUT_MS:-200}
      - LANGFUSE_HOST=${LANGFUSE_HOST}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - ENV_FILE=/app/.env
      - RUNS_DIR=.runs
      - ARTIFACTS_DIR=.runs/artifacts
      - TUNER_STATE_PATH=.runs/tuner_state.json
      - WORKER_URLS=${WORKER_URLS:-http://gpu-worker:8090}
      - AUTOTUNER_RPS=${AUTOTUNER_RPS:-0}
    ports:
      - "8000:8000"
    depends_on: [ qdrant, milvus-standalone ]
    volumes:
      - ~/ssx-lab/models:/app/models
      - ./models:/app/models
      - ./modules:/app/modules
      - ./experiments:/app/experiments:ro
      - ./configs:/app/configs:ro
      - ./services/fiqa_api:/app/services/fiqa_api:ro
      - ./mcp:/app/mcp:ro
      - ./data:/app/data:ro
      - ./reports:/app/reports
      - ./.runs:/app/.runs
      - ./baselines:/app/baselines
      - ./.env.current:/app/.env:ro
    # CPU-only service - no GPU runtime
  gpu-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.gpu-worker
    environment:
      - HF_HOME=/models/hf
      - TORCH_HOME=/models/torch
      - MODEL_EMBED=${MODEL_EMBED:-sentence-transformers/all-MiniLM-L6-v2}
      - MODEL_RERANK=${MODEL_RERANK:-cross-encoder/ms-marco-MiniLM-L-12-v2}
      - MAX_CONCURRENCY=${GPU_MAX_CONCURRENCY:-2}
      - QUEUE_LIMIT=${GPU_QUEUE_LIMIT:-128}
      - BATCH_WINDOW_MS=${GPU_BATCH_WINDOW_MS:-20}
      - PORT=8090
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    ports:
      - "8090:8090"
    volumes:
      - ./cache/hf:/models/hf
      - ./cache/torch:/models/torch
      - ./.models:/models
    healthcheck:
      test: [ "CMD", "curl", "-fsS", "http://localhost:8090/ready" ]
      interval: 5s
      timeout: 3s
      start_period: 60s
      retries: 30
  auto-tuner:
    build:
      context: .
      dockerfile: services/auto_tuner/Dockerfile
    restart: unless-stopped
    environment:
      - AUTOTUNER_ENABLED=${AUTOTUNER_ENABLED:-true}
    depends_on: [ rag-api ]
  gpu-smoke:
    image: nvidia/cuda:12.3.1-base-ubuntu22.04
    command: [ "nvidia-smi", "-L" ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
volumes:
  redis_data:
  milvus_etcd:
  milvus_minio:
  milvus_data:
