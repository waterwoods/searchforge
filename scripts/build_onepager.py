#!/usr/bin/env python3
"""
ç”Ÿæˆ AutoTuner ä¸€é¡µæˆæœæŠ¥å‘Š
åŒ…å«ä¸‰ç»´åº¦è¯„ä¼°ï¼ˆè´¨é‡/SLA/æˆæœ¬ï¼‰å’Œæ—¶åºæ›²çº¿
è¾“å‡º Markdown å’Œ PDF
"""

import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple


def evaluate_quality(metrics: Dict) -> Tuple[str, str]:
    """
    è¯„ä¼°è´¨é‡ç»´åº¦
    è¿”å›: (é¢œè‰²ç­‰çº§, è¯„ä¼°æ–‡æœ¬)
    """
    issues = []
    color = 'green'
    
    # è§„åˆ™1: p-value < 0.05 æ˜¾è‘—æ€§
    if metrics['p_value'] < 0.05:
        issues.append('âœ… ç»Ÿè®¡æ˜¾è‘— (p<0.05)')
    else:
        issues.append('âš ï¸ ç»Ÿè®¡ä¸æ˜¾è‘—')
        color = 'yellow'
    
    # è§„åˆ™2: è‡³å°‘10ä¸ªæ¡¶
    if metrics['buckets'] >= 10:
        issues.append(f'âœ… æ ·æœ¬å……è¶³ ({metrics["buckets"]}æ¡¶)')
    else:
        issues.append(f'âš ï¸ æ ·æœ¬ä¸è¶³ ({metrics["buckets"]}æ¡¶)')
        color = 'yellow'
    
    # è§„åˆ™3: å¬å›ç‡æå‡
    if metrics['delta_recall'] > 0.01:
        issues.append(f'âœ… å¬å›ç‡æ˜¾è‘—æå‡ (+{metrics["delta_recall"]:.3f})')
    elif metrics['delta_recall'] >= -0.01:
        issues.append(f'âœ“ å¬å›ç‡ç¨³å®š ({metrics["delta_recall"]:.3f})')
    else:
        issues.append(f'âŒ å¬å›ç‡ä¸‹é™ ({metrics["delta_recall"]:.3f})')
        color = 'red'
    
    return color, '\n'.join(issues)


def evaluate_sla(metrics: Dict) -> Tuple[str, str]:
    """
    è¯„ä¼° SLA ç»´åº¦
    è¿”å›: (é¢œè‰²ç­‰çº§, è¯„ä¼°æ–‡æœ¬)
    """
    issues = []
    color = 'green'
    
    # è§„åˆ™1: Î”P95 <= +5ms (å…è®¸å°å¹…å¢åŠ )
    if metrics['delta_p95_ms'] <= 5:
        issues.append(f'âœ… P95å»¶è¿Ÿä¼˜ç§€ ({metrics["delta_p95_ms"]:.1f}ms)')
    elif metrics['delta_p95_ms'] <= 20:
        issues.append(f'âœ“ P95å»¶è¿Ÿå¯æ¥å— (+{metrics["delta_p95_ms"]:.1f}ms)')
        color = 'yellow' if color == 'green' else color
    else:
        issues.append(f'âŒ P95å»¶è¿Ÿè¿‡é«˜ (+{metrics["delta_p95_ms"]:.1f}ms)')
        color = 'red'
    
    # è§„åˆ™2: Safety rate >= 0.99
    if metrics['safety_rate'] >= 0.99:
        issues.append(f'âœ… å®‰å…¨ç‡ä¼˜ç§€ ({metrics["safety_rate"]:.3f})')
    elif metrics['safety_rate'] >= 0.95:
        issues.append(f'âœ“ å®‰å…¨ç‡è‰¯å¥½ ({metrics["safety_rate"]:.3f})')
        color = 'yellow' if color == 'green' else color
    else:
        issues.append(f'âŒ å®‰å…¨ç‡ä¸è¶³ ({metrics["safety_rate"]:.3f})')
        color = 'red'
    
    # è§„åˆ™3: Apply rate >= 0.95
    if metrics['apply_rate'] >= 0.95:
        issues.append(f'âœ… åº”ç”¨ç‡ä¼˜ç§€ ({metrics["apply_rate"]:.3f})')
    elif metrics['apply_rate'] >= 0.90:
        issues.append(f'âœ“ åº”ç”¨ç‡è‰¯å¥½ ({metrics["apply_rate"]:.3f})')
    else:
        issues.append(f'âš ï¸ åº”ç”¨ç‡åä½ ({metrics["apply_rate"]:.3f})')
        color = 'yellow' if color == 'green' else color
    
    return color, '\n'.join(issues)


def evaluate_cost(metrics: Dict) -> Tuple[str, str]:
    """
    è¯„ä¼°æˆæœ¬ç»´åº¦
    è¿”å›: (é¢œè‰²ç­‰çº§, è¯„ä¼°æ–‡æœ¬)
    """
    issues = []
    cost = metrics['cost_per_query']
    
    # æˆæœ¬é˜ˆå€¼
    if cost <= 0.00005:
        color = 'green'
        issues.append(f'âœ… æˆæœ¬ä¼˜ç§€ (${cost:.6f}/æŸ¥è¯¢)')
    elif cost <= 0.0001:
        color = 'yellow'
        issues.append(f'âœ“ æˆæœ¬å¯æ¥å— (${cost:.6f}/æŸ¥è¯¢)')
    else:
        color = 'red'
        issues.append(f'âŒ æˆæœ¬è¿‡é«˜ (${cost:.6f}/æŸ¥è¯¢)')
    
    # æœˆåº¦æˆæœ¬ä¼°ç®— (å‡è®¾1MæŸ¥è¯¢/æœˆ)
    monthly_cost = cost * 1_000_000
    issues.append(f'ğŸ“Š æœˆåº¦ä¼°ç®—: ${monthly_cost:.2f} (1MæŸ¥è¯¢)')
    
    return color, '\n'.join(issues)


def overall_verdict(quality_color: str, sla_color: str, cost_color: str) -> str:
    """
    ç»¼åˆåˆ¤å®š
    å…¨ç»¿=PASSï¼Œæœ‰é»„æ— çº¢=WARNï¼Œå«çº¢=FAIL
    """
    colors = [quality_color, sla_color, cost_color]
    
    if all(c == 'green' for c in colors):
        return 'PASS'
    elif 'red' in colors:
        return 'FAIL'
    else:
        return 'WARN'


def generate_markdown_report(data: Dict, plots_info: Dict, output_path: Path):
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    scenarios = data['scenarios']
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    md = f"""# AutoTuner ä¸€é¡µæˆæœå¡ï¼š0â€“1 å°æ—¶å¬å›ç‡è¿½è¸ª

**ç”Ÿæˆæ—¶é—´**: {timestamp}  
**æ•°æ®æ¥æº**: {data['source_dir']}  
**å®éªŒæ¨¡å¼**: {scenarios['A']['mode'].upper()}  
**åœºæ™¯æ•°é‡**: {len(scenarios)}

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šæ±‡æ€»äº† AutoTuner åœ¨ä¸‰ä¸ªæ ¸å¿ƒåœºæ™¯ï¼ˆA/B/Cï¼‰ä¸‹çš„å®éªŒè¡¨ç°ï¼ŒåŸºäº 0-1 å°æ—¶çš„å®æ—¶æ•°æ®ï¼Œä»**è´¨é‡**ã€**SLA** å’Œ**æˆæœ¬**ä¸‰ä¸ªç»´åº¦è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚

### åœºæ™¯æ¦‚è§ˆ

| åœºæ™¯ | é¢„è®¾é…ç½® | æ—¶é•¿ | æ¡¶æ•° | åˆ¤å®š |
|:----:|:--------|:----:|:----:|:----:|
"""
    
    # ä¸ºæ¯ä¸ªåœºæ™¯è®¡ç®—åˆ¤å®š
    verdicts = {}
    for scenario_key in sorted(scenarios.keys()):
        metrics = scenarios[scenario_key]
        q_color, _ = evaluate_quality(metrics)
        s_color, _ = evaluate_sla(metrics)
        c_color, _ = evaluate_cost(metrics)
        verdict = overall_verdict(q_color, s_color, c_color)
        verdicts[scenario_key] = verdict
        
        verdict_emoji = {'PASS': 'âœ…', 'WARN': 'âš ï¸', 'FAIL': 'âŒ'}[verdict]
        md += f"| **{scenario_key}** | {metrics['preset']} | {metrics['duration_sec']}s | {metrics['buckets']} | {verdict_emoji} **{verdict}** |\n"
    
    md += "\n---\n\n"
    
    # ä¸‰ç»´åº¦è¯„ä¼°è¯¦æƒ…
    md += "## ğŸ¯ ä¸‰ç»´åº¦åˆå¹¶è¯„ä¼°\n\n"
    
    for scenario_key in sorted(scenarios.keys()):
        metrics = scenarios[scenario_key]
        verdict = verdicts[scenario_key]
        
        md += f"### åœºæ™¯ {scenario_key}: {metrics['preset']}\n\n"
        md += f"**ç»¼åˆåˆ¤å®š**: **{verdict}**\n\n"
        
        # è´¨é‡å¡
        q_color, q_text = evaluate_quality(metrics)
        md += f"#### ğŸ¯ è´¨é‡ç»´åº¦ ({q_color.upper()})\n\n"
        md += f"```\n{q_text}\n```\n\n"
        
        # SLAå¡
        s_color, s_text = evaluate_sla(metrics)
        md += f"#### âš¡ SLAç»´åº¦ ({s_color.upper()})\n\n"
        md += f"```\n{s_text}\n```\n\n"
        
        # æˆæœ¬å¡
        c_color, c_text = evaluate_cost(metrics)
        md += f"#### ğŸ’° æˆæœ¬ç»´åº¦ ({c_color.upper()})\n\n"
        md += f"```\n{c_text}\n```\n\n"
        
        # æ—¶åºæ›²çº¿
        if scenario_key in plots_info:
            plot_info = plots_info[scenario_key]
            recall_plot = Path(plot_info['recall_plot']).name
            p95_plot = Path(plot_info['p95_plot']).name
            
            md += f"#### ğŸ“ˆ æ—¶åºæ›²çº¿\n\n"
            md += f"**å¬å›ç‡è¶‹åŠ¿ (0-{plot_info['duration']//60}åˆ†é’Ÿ)**\n\n"
            md += f"![Recall]({recall_plot})\n\n"
            md += f"**P95å»¶è¿Ÿè¶‹åŠ¿ (0-{plot_info['duration']//60}åˆ†é’Ÿ)**\n\n"
            md += f"![P95]({p95_plot})\n\n"
        
        md += "---\n\n"
    
    # æ€»ç»“
    md += "## ğŸ‰ æ€»ç»“\n\n"
    
    pass_count = sum(1 for v in verdicts.values() if v == 'PASS')
    warn_count = sum(1 for v in verdicts.values() if v == 'WARN')
    fail_count = sum(1 for v in verdicts.values() if v == 'FAIL')
    
    md += f"- âœ… **é€šè¿‡**: {pass_count} ä¸ªåœºæ™¯\n"
    md += f"- âš ï¸ **è­¦å‘Š**: {warn_count} ä¸ªåœºæ™¯\n"
    md += f"- âŒ **å¤±è´¥**: {fail_count} ä¸ªåœºæ™¯\n\n"
    
    overall = 'PASS' if fail_count == 0 and warn_count == 0 else ('WARN' if fail_count == 0 else 'FAIL')
    md += f"**æ€»ä½“åˆ¤å®š**: **{overall}**\n\n"
    
    # å…³é”®æ•°å­—
    avg_delta_recall = sum(m['delta_recall'] for m in scenarios.values()) / len(scenarios)
    avg_delta_p95 = sum(m['delta_p95_ms'] for m in scenarios.values()) / len(scenarios)
    avg_cost = sum(m['cost_per_query'] for m in scenarios.values()) / len(scenarios)
    
    md += f"### å…³é”®æ•°å­—\n\n"
    md += f"- å¹³å‡å¬å›ç‡æå‡: **+{avg_delta_recall:.3f}**\n"
    md += f"- å¹³å‡P95å˜åŒ–: **+{avg_delta_p95:.1f} ms**\n"
    md += f"- å¹³å‡æ¯æŸ¥è¯¢æˆæœ¬: **${avg_cost:.6f}**\n"
    md += f"- æ€»å®éªŒæ¡¶æ•°: **{sum(m['buckets'] for m in scenarios.values())}**\n\n"
    
    md += "---\n\n"
    md += "*æœ¬æŠ¥å‘Šç”± AutoTuner è‡ªåŠ¨ç”Ÿæˆ*\n"
    
    # å†™å…¥æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md)
    
    return overall, {
        'avg_delta_recall': avg_delta_recall,
        'avg_delta_p95': avg_delta_p95,
        'avg_cost': avg_cost,
        'pass_count': pass_count,
        'warn_count': warn_count,
        'fail_count': fail_count
    }


def main():
    """ä¸»å‡½æ•°"""
    # è¯»å–æ•°æ®
    base_dir = Path(__file__).parent.parent / 'docs'
    metrics_path = base_dir / 'collected_metrics.json'
    plots_path = base_dir / 'plots' / 'plots_info.json'
    
    if not metrics_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°æŒ‡æ ‡æ–‡ä»¶: {metrics_path}")
        return
    
    with open(metrics_path, 'r') as f:
        data = json.load(f)
    
    plots_info = {}
    if plots_path.exists():
        with open(plots_path, 'r') as f:
            plots_info = json.load(f)
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    output_md = base_dir / 'RESULTS_SUMMARY.md'
    print(f"ğŸ“ ç”Ÿæˆ Markdown æŠ¥å‘Š...")
    verdict, summary = generate_markdown_report(data, plots_info, output_md)
    print(f"   âœ… {output_md}")
    
    # æ‰“å°ç»ˆç«¯æ‘˜è¦
    total_buckets = sum(m['buckets'] for m in data['scenarios'].values())
    print(f"\n{'='*60}")
    print(f"[æ›²çº¿] {'/'.join(sorted(data['scenarios'].keys()))} æ›²çº¿å·²ç”Ÿæˆï¼ˆæ€»æ¡¶æ•°: {total_buckets}ï¼‰")
    print(f"[æ±‡æ€»] Verdict={verdict} | Î”Recall={summary['avg_delta_recall']:.3f} | Î”P95={summary['avg_delta_p95']:.1f}ms | æˆæœ¬=${summary['avg_cost']:.6f}")
    print(f"{'='*60}")
    
    return output_md


if __name__ == '__main__':
    main()

