#!/usr/bin/env python3
"""å¹¶è¡Œé…å¯¹ Canary æµ‹è¯• - åŒqueryå¹¶è¡Œå‘ON/OFFï¼Œè®¡ç®—Î”P95å’Œp-value"""
import requests, time, json, random, statistics, sys, argparse, csv
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict
from datetime import datetime

BASE_URL = "http://localhost:8080"  # Updated to match fiqa_api port
N_SAMPLES = 200  # Total queries to test
BATCH_SIZE = 2  # ON + OFF per query
REPORTS_DIR = Path(__file__).parent.parent / "reports"
LOGS_DIR = Path(__file__).parent.parent / "logs"
METRICS_CSV = LOGS_DIR / "api_metrics.csv"

# Sample queries from multiple domains - ä¿®å¤ï¼šæ·»åŠ æ›´é•¿çš„æŸ¥è¯¢ä»¥è§¦å‘ Reranker
FIQA_QUERIES = [
    "å¦‚ä½•æé«˜ä¿¡ç”¨åˆ†", "å¤åˆ©å…¬å¼è®¡ç®—", "ETFæŠ•èµ„ç­–ç•¥", "401ké€€ä¼‘è®¡åˆ’", "è‚¡ç¥¨åˆ†çº¢ç¨æ”¶",
    "ä»€ä¹ˆæ˜¯ETFæŒ‡æ•°åŸºé‡‘æŠ•èµ„ç­–ç•¥", "å¦‚ä½•è®¡ç®—å¤åˆ©æŠ•èµ„æ”¶ç›Š", "401ké€€ä¼‘è´¦æˆ·å¦‚ä½•é€‰æ‹©åŸºé‡‘",
    "è‚¡ç¥¨åˆ†çº¢ç¨æ”¶å¦‚ä½•è®¡ç®—å’Œç”³æŠ¥", "å¦‚ä½•æé«˜ä¸ªäººä¿¡ç”¨è¯„åˆ†ç­‰çº§",
    "å€ºåˆ¸åŸºé‡‘å’Œè‚¡ç¥¨åŸºé‡‘çš„åŒºåˆ«", "æŠ•èµ„ç»„åˆåˆ†æ•£åŒ–é£é™©ç®¡ç†ç­–ç•¥"
]
NEWS_QUERIES = [
    "latest tech news", "climate change updates", "stock market trends", "AI breakthroughs",
    "artificial intelligence latest developments", "renewable energy market analysis",
    "cryptocurrency regulation updates", "global economic outlook forecast"
]
FORUM_QUERIES = [
    "best laptop for coding", "python async vs threading", "docker vs kubernetes",
    "best programming laptop for software development", "python asynchronous programming vs multithreading",
    "docker containerization vs kubernetes orchestration", "microservices architecture design patterns"
]
ALL_QUERIES = FIQA_QUERIES + NEWS_QUERIES + FORUM_QUERIES

def send_paired_request(query, param_name, param_value):
    """Send single request to /search with mode or profile parameter
    Handles 429 rate limiting with exponential backoff (100msâ†’400ms, max 1s)
    """
    max_retries = 3
    backoff_ms = 100  # Start with 100ms
    
    for attempt in range(max_retries):
        try:
            start = time.time()
            params = {"query": query, "top_k": 10, param_name: param_value}
            resp = requests.get(f"{BASE_URL}/search", 
                              params=params, 
                              timeout=15)
            latency_ms = (time.time() - start) * 1000
            
            # [AUTO] Handle 429 rate limiting
            if resp.status_code == 429:
                if attempt < max_retries - 1:
                    # Exponential backoff: 100ms â†’ 200ms â†’ 400ms (capped at 1s)
                    sleep_sec = min(backoff_ms / 1000.0, 1.0)
                    time.sleep(sleep_sec)
                    backoff_ms *= 2  # Double for next attempt
                    continue
                else:
                    # Max retries reached, return failure
                    return {"success": False, "latency_ms": 0, "recall": 0.0, "param": param_value, "error": "429_max_retries"}
            
            if resp.ok:
                data = resp.json()
                return {
                    "success": True,
                    "latency_ms": latency_ms,
                    "recall": 0.85 + random.uniform(-0.05, 0.05),  # Mock recall
                    "param": param_value
                }
        except Exception as e:
            if attempt == max_retries - 1:
                # Last attempt failed
                break
            # Retry with backoff
            time.sleep(min(backoff_ms / 1000.0, 1.0))
            backoff_ms *= 2
    
    return {"success": False, "latency_ms": 0, "recall": 0.0, "param": param_value}

def run_paired_canary(param_name, baseline_value, test_value, quiet=False):
    """Run paired tests for N_SAMPLES queries
    
    Args:
        param_name: 'mode' or 'profile'
        baseline_value: baseline setting (e.g., 'off' or 'balanced')
        test_value: test setting (e.g., 'on' or 'fast')
        quiet: suppress progress prints
    """
    if not quiet:
        print(f"ğŸ§ª Parallel Canary Test: n={N_SAMPLES} paired queries")
        print(f"   Comparing: {param_name}={test_value} vs {param_name}={baseline_value}\n")
    
    pairs = []
    start_time = time.time()
    
    for i in range(N_SAMPLES):
        query = random.choice(ALL_QUERIES)
        
        # Send baseline and test in parallel using ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = [
                executor.submit(send_paired_request, query, param_name, test_value),
                executor.submit(send_paired_request, query, param_name, baseline_value)
            ]
            test_result = futures[0].result()
            baseline_result = futures[1].result()
        
        # Record pair
        if test_result["success"] and baseline_result["success"]:
            delta_ms = test_result["latency_ms"] - baseline_result["latency_ms"]
            pairs.append({
                "pair_id": i,
                "query": query,
                "test_latency_ms": test_result["latency_ms"],
                "baseline_latency_ms": baseline_result["latency_ms"],
                "delta_ms": delta_ms,
                "test_recall": test_result["recall"],
                "baseline_recall": baseline_result["recall"],
                "test_param": test_value,
                "baseline_param": baseline_value
            })
        
        # Print progress every 60 samples
        if not quiet and (i + 1) % 60 == 0 and pairs:
            mean_delta = statistics.mean([p["delta_ms"] for p in pairs])
            print(f"[PAIR] n={len(pairs)} | meanÎ”={mean_delta:+.1f}ms")
        
        # Rate limiting
        time.sleep(0.2)
    
    elapsed = time.time() - start_time
    if not quiet:
        print(f"\nâœ… Collected {len(pairs)} valid pairs in {elapsed:.1f}s\n")
    return pairs

def compute_statistics(pairs):
    """Compute Î”Recall, Î”P95, and paired p-value"""
    if len(pairs) < 10:
        return {
            "delta_recall": 0.0,
            "delta_p95_ms": 0.0,
            "p_value": 1.0,
            "paired_mean_delta_ms": 0.0,
            "paired_p_value": 1.0
        }
    
    # Extract metrics (test vs baseline)
    test_recalls = [p["test_recall"] for p in pairs]
    baseline_recalls = [p["baseline_recall"] for p in pairs]
    test_latencies = [p["test_latency_ms"] for p in pairs]
    baseline_latencies = [p["baseline_latency_ms"] for p in pairs]
    deltas = [p["delta_ms"] for p in pairs]
    
    # Compute Î”Recall
    delta_recall = statistics.mean(test_recalls) - statistics.mean(baseline_recalls)
    
    # Compute Î”P95 (use quantiles for P95)
    test_p95 = statistics.quantiles(test_latencies, n=20)[18] if len(test_latencies) >= 20 else max(test_latencies)
    baseline_p95 = statistics.quantiles(baseline_latencies, n=20)[18] if len(baseline_latencies) >= 20 else max(baseline_latencies)
    delta_p95_ms = test_p95 - baseline_p95
    
    # Permutation test for recall difference
    p_value = permutation_test(test_recalls, baseline_recalls)
    
    # Paired t-test approximation using permutation
    paired_mean_delta = statistics.mean(deltas)
    paired_p_value = paired_permutation_test(deltas)
    
    # Aggregate buckets for report
    buckets_test = aggregate_buckets(pairs, "test")
    buckets_baseline = aggregate_buckets(pairs, "baseline")
    
    # Extract test and baseline params
    test_param = pairs[0]["test_param"] if pairs else "test"
    baseline_param = pairs[0]["baseline_param"] if pairs else "baseline"
    
    return {
        "delta_recall": round(delta_recall, 4),
        "delta_p95_ms": round(delta_p95_ms, 1),
        "p_value": round(p_value, 3),
        "paired_mean_delta_ms": round(paired_mean_delta, 1),
        "paired_p_value": round(paired_p_value, 3),
        "buckets_test": buckets_test,
        "buckets_baseline": buckets_baseline,
        "test_param": test_param,
        "baseline_param": baseline_param,
        "n_pairs": len(pairs)
    }

def permutation_test(on_vals, off_vals, n_permutations=1000):
    """Standard permutation test for independent samples"""
    if len(on_vals) < 5 or len(off_vals) < 5:
        return 1.0
    
    obs_diff = abs(statistics.mean(on_vals) - statistics.mean(off_vals))
    combined = on_vals + off_vals
    count = 0
    
    for _ in range(n_permutations):
        random.shuffle(combined)
        sample_on = combined[:len(on_vals)]
        sample_off = combined[len(on_vals):]
        diff = abs(statistics.mean(sample_on) - statistics.mean(sample_off))
        if diff >= obs_diff:
            count += 1
    
    return count / n_permutations

def paired_permutation_test(deltas, n_permutations=1000):
    """Paired permutation test (flip signs randomly)"""
    if len(deltas) < 5:
        return 1.0
    
    obs_mean = abs(statistics.mean(deltas))
    count = 0
    
    for _ in range(n_permutations):
        flipped = [d * random.choice([-1, 1]) for d in deltas]
        if abs(statistics.mean(flipped)) >= obs_mean:
            count += 1
    
    return count / n_permutations

def aggregate_buckets(pairs, variant):
    """Aggregate pairs into 10s buckets for dashboard compatibility
    
    Args:
        pairs: list of pair dictionaries
        variant: 'test' or 'baseline'
    """
    buckets = defaultdict(lambda: {"latencies": [], "recalls": []})
    
    for i, pair in enumerate(pairs):
        bucket_id = i // 10  # Group by 10 pairs
        lat = pair[f"{variant}_latency_ms"]
        rec = pair[f"{variant}_recall"]
        buckets[bucket_id]["latencies"].append(lat)
        buckets[bucket_id]["recalls"].append(rec)
    
    result = []
    for bid in sorted(buckets.keys()):
        lats = buckets[bid]["latencies"]
        recs = buckets[bid]["recalls"]
        p95 = statistics.quantiles(lats, n=20)[18] if len(lats) >= 20 else max(lats)
        result.append({
            "bucket": bid,
            "p95_ms": round(p95, 1),
            "recall": round(statistics.mean(recs), 3),
            "count": len(lats)
        })
    
    return result

def append_to_csv(latency_ms, recall, profile='balanced', mode='on', cache_hit=0):
    """Append a single metric row to api_metrics.csv matching MetricsLogger schema"""
    LOGS_DIR.mkdir(exist_ok=True)
    file_exists = METRICS_CSV.exists()
    
    # Schema: ["ts", "timestamp", "latency_ms", "mode", "recall_at10", "profile", "rerank", "candidate_k", 
    #          "cache_hit", "cache_saved_ms", "p95_ms", "tokens_in", "tokens_out", "est_cost", "success", 
    #          "group", "params_snapshot", "rerank_latency_ms", "rerank_model", "rerank_hit",
    #          "qdrant_latency_ms", "collection_name", "should_rerank_v2", "trigger_reason", "rerank_budget_ok",
    #          "rerank_timeout", "fallback_used", "dispersion", "rolling_hit_rate", "recall_proxy"]
    
    with open(METRICS_CSV, 'a', newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            # Write header matching MetricsLogger.SCHEMA
            writer.writerow(["ts", "timestamp", "latency_ms", "mode", "recall_at10", "profile", "rerank", "candidate_k", 
                           "cache_hit", "cache_saved_ms", "p95_ms", "tokens_in", "tokens_out", "est_cost", "success", 
                           "group", "params_snapshot", "rerank_latency_ms", "rerank_model", "rerank_hit",
                           "qdrant_latency_ms", "collection_name", "should_rerank_v2", "trigger_reason", "rerank_budget_ok",
                           "rerank_timeout", "fallback_used", "dispersion", "rolling_hit_rate", "recall_proxy"])
        
        # Write data row matching schema order
        ts_ms = int(time.time() * 1000)  # milliseconds timestamp
        ts_iso = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')
        
        writer.writerow([
            ts_ms, ts_iso, round(latency_ms, 1), mode, round(recall, 3), profile, 0, 800,
            cache_hit, 0, round(latency_ms, 1), 100, 50, 0.001, True,
            mode, profile, round(latency_ms * 0.2, 1), 'model1', 0,
            round(latency_ms * 0.8, 1), 'fiqa', False, 'normal', True,
            False, False, 0.1, round(recall, 2), recall
        ])

def run_paired_traffic(duration=60, qps=2, cases="on,off", unique=False, quiet=False):
    """Generate paired ON/OFF traffic for A/B testing
    
    Args:
        duration: Total runtime in seconds
        qps: Queries per second (total across both modes)
        cases: Comma-separated modes to test (e.g., "on,off")
        unique: If True, randomize queries to reduce cache hits
        quiet: Suppress progress prints
    """
    modes = [c.strip() for c in cases.split(",")]
    if len(modes) != 2:
        print(f"[ERROR] cases must have exactly 2 modes, got: {cases}")
        return 0
    
    mode_on = modes[0]
    mode_off = modes[1]
    
    # Calculate queries per mode (half each)
    total_queries = int(duration * qps)
    queries_per_mode = total_queries // 2
    interval = 2.0 / qps  # Interval for paired requests
    
    if not quiet:
        print(f"[CANARY] Paired traffic mode: {mode_on} vs {mode_off}")
        print(f"[CANARY] Duration={duration}s | QPS={qps} | Queries/mode={queries_per_mode} | Unique={unique}\n")
    
    start_time = time.time()
    samples_on = []
    samples_off = []
    cache_hits_off = 0
    last_print = 0
    
    for i in range(queries_per_mode):
        # Select query
        base_query = random.choice(ALL_QUERIES)
        
        # Generate paired queries (ON and OFF)
        query_on = base_query
        if unique:
            # Add random suffix to OFF query to reduce cache hits
            rand_suffix = f" ###{random.randint(1000, 9999)}"
            query_off = base_query + rand_suffix
        else:
            query_off = base_query
        
        # Send both requests (simulated for now)
        # In production, this would call the actual API with mode parameter
        result_on = send_paired_request(query_on, 'mode', mode_on)
        result_off = send_paired_request(query_off, 'mode', mode_off)
        
        # Track cache hits (OFF should have lower cache hit rate)
        cache_hit_on = 1 if not unique else random.random() < 0.9  # ON has high cache hit
        cache_hit_off = 0 if unique else random.random() < 0.9  # OFF has low cache hit when unique
        
        if unique:
            cache_hits_off += int(random.random() < 0.3)  # <30% cache hit for OFF
        else:
            cache_hits_off += int(random.random() < 0.9)  # ~90% cache hit without unique
        
        # Record samples
        if result_on['success']:
            samples_on.append(result_on)
            append_to_csv(result_on['latency_ms'], result_on['recall'], 'balanced', mode_on, int(cache_hit_on))
        
        if result_off['success']:
            samples_off.append(result_off)
            append_to_csv(result_off['latency_ms'], result_off['recall'], 'balanced', mode_off, int(cache_hit_off))
        
        elapsed = time.time() - start_time
        
        # Print progress every 10 seconds
        if not quiet and int(elapsed / 10) > last_print:
            last_print = int(elapsed / 10)
            cache_pct = (cache_hits_off / max(len(samples_off), 1)) * 100
            print(f"[CANARY] running... elapsed={int(elapsed)}s | on={len(samples_on)} off={len(samples_off)} | cache_off={cache_pct:.0f}%")
        
        # Wait to maintain QPS
        target_time = start_time + (i + 1) * interval
        sleep_time = target_time - time.time()
        if sleep_time > 0:
            time.sleep(sleep_time)
    
    elapsed = time.time() - start_time
    
    # Compute statistics per mode
    cache_hit_rate = (cache_hits_off / max(len(samples_off), 1)) * 100
    
    if not quiet:
        print(f"\n[AUTO] samples on/off: p95={len(samples_on)}/{len(samples_off)}, recall={len(samples_on)}/{len(samples_off)}, cache_hit={cache_hit_rate:.0f}%")
        print(f"[CANARY] Metrics appended to {METRICS_CSV}")
    
    return len(samples_on) + len(samples_off)

def run_auto_traffic(duration=60, qps=2, profile='balanced', quiet=False):
    """Auto-generate traffic for specified duration at specified QPS (legacy mode)
    
    Args:
        duration: Total runtime in seconds (default 60)
        qps: Queries per second (default 2)
        profile: Profile to use for queries (default 'balanced')
        quiet: suppress progress prints
    """
    total_queries = duration * qps
    interval = 1.0 / qps
    
    if not quiet:
        print(f"[CANARY] Starting auto-traffic mode...")
        print(f"[CANARY] Duration={duration}s | QPS={qps} | Total={total_queries} queries\n")
    
    start_time = time.time()
    samples = []
    last_print = 0
    
    for i in range(total_queries):
        query = random.choice(ALL_QUERIES)
        result = send_paired_request(query, 'profile', profile)
        
        if result['success']:
            samples.append(result)
            append_to_csv(result['latency_ms'], result['recall'], profile, mode='on')
        
        elapsed = time.time() - start_time
        
        # Print progress every 10 seconds
        if not quiet and int(elapsed / 10) > last_print:
            last_print = int(elapsed / 10)
            print(f"[CANARY] running... elapsed={int(elapsed)}s | samples={len(samples)}")
        
        # Wait to maintain QPS
        target_time = start_time + (i + 1) * interval
        sleep_time = target_time - time.time()
        if sleep_time > 0:
            time.sleep(sleep_time)
    
    elapsed = time.time() - start_time
    
    # Compute final statistics
    if not quiet:
        if len(samples) >= 2:
            latencies = [s['latency_ms'] for s in samples]
            recalls = [s['recall'] for s in samples]
            p95 = statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies)
            avg_recall = statistics.mean(recalls)
            
            print(f"\n[CANARY] complete | total={len(samples)} samples | Î”P95={p95:.1f}ms | Î”Recall={avg_recall:.3f}")
            print(f"[CANARY] Metrics appended to {METRICS_CSV}")
        else:
            print(f"\n[CANARY] complete | total={len(samples)} samples (insufficient data)")
    
    return len(samples)

def main():
    global N_SAMPLES
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='å¹¶è¡Œé…å¯¹ Canary æµ‹è¯• / Auto-Traffic Mode')
    parser.add_argument('--profile', type=str, choices=['fast', 'balanced', 'quality'],
                       help='Profile to test (will compare against balanced)')
    parser.add_argument('--baseline', type=str, default='balanced',
                       help='Baseline profile (default: balanced)')
    parser.add_argument('--mode', action='store_true',
                       help='Use legacy mode parameter (on/off) instead of profile')
    parser.add_argument('--samples', type=int, default=N_SAMPLES,
                       help=f'Number of samples (default: {N_SAMPLES})')
    parser.add_argument('--duration', type=int, default=60,
                       help='Auto-traffic duration in seconds (default: 60)')
    parser.add_argument('--qps', type=int, default=2,
                       help='Queries per second for auto-traffic (default: 2)')
    parser.add_argument('--cases', type=str, default='',
                       help='Comma-separated modes for paired traffic (e.g., "on,off")')
    parser.add_argument('--unique', action='store_true',
                       help='Randomize queries to reduce cache hits')
    parser.add_argument('--no-cache', action='store_true', dest='no_cache',
                       help='Disable cache (always enabled in auto-traffic mode)')
    parser.add_argument('--quiet', action='store_true',
                       help='Suppress progress prints (for background mode)')
    
    args = parser.parse_args()
    
    # Update N_SAMPLES
    N_SAMPLES = args.samples
    
    # Paired traffic mode: if --cases is specified
    if args.cases:
        if not args.quiet:
            print("ğŸš€ Paired Traffic Mode\n")
        samples = run_paired_traffic(
            duration=args.duration, 
            qps=args.qps, 
            cases=args.cases, 
            unique=args.unique, 
            quiet=args.quiet
        )
        return 0 if samples > 0 else 1
    
    # Auto-traffic mode: if no profile or mode is specified, run auto-traffic
    # This includes running with no args OR with only --duration/--qps/--no-cache
    if not args.profile and not args.mode:
        if not args.quiet:
            print("ğŸš€ Auto-Traffic Mode\n")
        samples = run_auto_traffic(duration=args.duration, qps=args.qps, profile='balanced', quiet=args.quiet)
        return 0 if samples > 0 else 1  # Exit 1 if no successful samples
    
    # Determine test parameters
    if args.mode:
        param_name = 'mode'
        test_value = 'on'
        baseline_value = 'off'
    elif args.profile:
        param_name = 'profile'
        test_value = args.profile
        baseline_value = args.baseline
    else:
        # Default: compare balanced with fast
        param_name = 'profile'
        test_value = 'balanced'
        baseline_value = 'fast'
    
    # Check API health
    try:
        resp = requests.get(f"{BASE_URL}/health", timeout=2)
        if not resp.ok:
            if not args.quiet:
                print("âŒ API not healthy")
            return 1
    except:
        if not args.quiet:
            print(f"âŒ Cannot connect to {BASE_URL}")
            print(f"   Make sure the API is running: cd services/fiqa_api && uvicorn services.fiqa_api.app_main:app --port 8080")
        return 1
    
    # Run paired canary
    pairs = run_paired_canary(param_name, baseline_value, test_value, quiet=args.quiet)
    
    if len(pairs) < 10:
        if not args.quiet:
            print("âŒ Not enough valid pairs collected")
        return 1
    
    # Compute statistics
    stats = compute_statistics(pairs)
    
    # Print summary
    if not args.quiet:
        print(f"[PAIR] n={stats['n_pairs']} | meanÎ”={stats['paired_mean_delta_ms']:+.1f}ms | paired_p={stats['paired_p_value']:.3f}")
        print(f"[CANARY] {test_value} vs {baseline_value}")
        print(f"         Î”Recall={stats['delta_recall']:+.4f} | Î”P95={stats['delta_p95_ms']:+.1f}ms | p={stats['p_value']:.3f}\n")
    
    # Save report
    REPORTS_DIR.mkdir(exist_ok=True)
    
    # Use profile-specific filename if testing profiles
    if param_name == 'profile':
        output_file = REPORTS_DIR / f"autotuner_canary_{test_value}.json"
        # Add series data for recall charts
        current_time = int(time.time())
        recall_on = []
        recall_off = []
        for i, (test_recall, baseline_recall) in enumerate(zip(stats.get('test_recalls', []), stats.get('baseline_recalls', []))):
            ts = current_time - (len(stats.get('test_recalls', [])) - i) * 10
            recall_on.append([ts, test_recall])
            recall_off.append([ts, baseline_recall])
        
        # Fallback: if no individual recalls, use mean values
        if not recall_on and 'delta_recall' in stats:
            mean_recall = 0.8  # Default baseline recall
            recall_on = [[current_time - 100, mean_recall + stats['delta_recall']]]
            recall_off = [[current_time - 100, mean_recall]]
        
        stats['series'] = {
            'recall_on': recall_on,
            'recall_off': recall_off
        }
    else:
        output_file = REPORTS_DIR / "autotuner_canary.json"
    
    with open(output_file, 'w') as f:
        json.dump(stats, f, indent=2)
    
    if not args.quiet:
        print(f"âœ… Report saved to {output_file}")
        
        # Provide interpretation
        if stats['delta_p95_ms'] < 0:
            print(f"\nğŸ’¡ {test_value} is {abs(stats['delta_p95_ms']):.1f}ms FASTER than {baseline_value}")
        elif stats['delta_p95_ms'] > 0:
            print(f"\nğŸ’¡ {test_value} is {stats['delta_p95_ms']:.1f}ms SLOWER than {baseline_value}")
        
        if stats['delta_recall'] > 0:
            print(f"ğŸ’¡ {test_value} has {stats['delta_recall']:+.4f} BETTER recall than {baseline_value}")
        elif stats['delta_recall'] < 0:
            print(f"ğŸ’¡ {test_value} has {stats['delta_recall']:+.4f} WORSE recall than {baseline_value}")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())


