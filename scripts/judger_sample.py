#!/usr/bin/env python3
"""
é‡‡æ ·å™¨ï¼šä»å†å²ç»“æœä¸­ç”Ÿæˆç›²æµ‹è¯„æµ‹æ‰¹æ¬¡
æ”¯æŒéšæœº+åˆ†å±‚é‡‡æ ·ç­–ç•¥ï¼Œè¾“å‡ºæˆå¯¹çš„ ON/OFF ç»“æœ
"""

import json
import random
import argparse
from pathlib import Path
from datetime import datetime


def load_latest_results():
    """åŠ è½½æœ€æ–°çš„ smoke æˆ– canary ç»“æœ"""
    reports_dir = Path(__file__).parent.parent / "reports"
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„ç»“æœæ–‡ä»¶
    candidates = [
        reports_dir / "fiqa_smoke_results.json",
        reports_dir / "canary_results.json",
    ]
    
    for path in candidates:
        if path.exists():
            with open(path) as f:
                return json.load(f)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("âš ï¸  æœªæ‰¾åˆ°å†å²ç»“æœï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º")
    return generate_mock_data()


def generate_mock_data():
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„æŸ¥è¯¢ç»“æœæ•°æ®"""
    topics = ["ETF", "401k", "mortgage", "credit score", "tax deduction"]
    queries = [
        "What is an ETF?",
        "How to maximize 401k contributions?",
        "Should I refinance my mortgage?",
        "How to improve credit score quickly?",
        "What are common tax deductions?",
        "Best investment strategies for beginners",
        "Difference between Roth and traditional IRA",
        "How does compound interest work?",
        "When should I start saving for retirement?",
        "What is a good debt to income ratio?",
        "How to calculate mortgage payments?",
        "What are index funds?",
        "How to diversify investment portfolio?",
        "What is dollar cost averaging?",
        "Should I pay off debt or invest?",
        "How much emergency fund do I need?",
        "What is the best budgeting method?",
        "How to negotiate salary increase?",
        "What are tax-advantaged accounts?",
        "How to plan for early retirement?",
    ]
    
    results = []
    for q in queries:
        # ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆ ON/OFF ä¸¤ç»„ç»“æœ
        results.append({
            "query": q,
            "topic": random.choice(topics),
            "query_length": len(q),
            "on_results": [
                {"title": f"ON Title {i+1}", "text": f"ON result {i+1} for: {q[:30]}...", "source": "mock", "score": 0.9 - i*0.1, "rerank_hit": 1} for i in range(3)
            ],
            "off_results": [
                {"title": f"OFF Title {i+1}", "text": f"OFF result {i+1} for: {q[:30]}...", "source": "mock", "score": 0.8 - i*0.1, "rerank_hit": 0} for i in range(3)
            ]
        })
    
    return results


def fetch_real_results(query: str, mode: str, top_k: int = 3) -> list:
    """è°ƒç”¨çœŸå® API è·å–ç»“æœï¼ˆbeir_fiqa_full_taï¼‰"""
    import requests
    import hashlib
    
    try:
        url = "http://localhost:8080/search"
        # POST request with JSON body for better compatibility
        payload = {"query": query, "top_k": top_k}
        params = {"mode": mode}  # mode as query param
        response = requests.post(url, json=payload, params=params, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            if "answers" in data:
                answers = data["answers"]
                # è½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼
                results = []
                for i, ans in enumerate(answers[:top_k]):
                    if isinstance(ans, str):
                        # ä½¿ç”¨æ–‡æœ¬å†…å®¹ç”Ÿæˆå”¯ä¸€ ID
                        doc_id = hashlib.md5(ans[:100].encode()).hexdigest()[:12]
                        results.append({
                            "id": doc_id,
                            "title": f"Result {i+1}",
                            "text": ans[:200],
                            "snippet": ans[:200],
                            "source": "API",
                            "score": 0.9-i*0.1,
                            "rank": i+1
                        })
                    elif isinstance(ans, dict):
                        text = ans.get("text", str(ans))[:200]
                        # ä½¿ç”¨æ–‡æœ¬å†…å®¹ç”Ÿæˆå”¯ä¸€ IDï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
                        doc_id = ans.get("id")
                        if not doc_id:
                            doc_id = hashlib.md5(text[:100].encode()).hexdigest()[:12]
                        results.append({
                            "id": doc_id,
                            "title": ans.get("title", f"Result {i+1}"),
                            "text": text,
                            "snippet": text,
                            "source": ans.get("source", "API"),
                            "score": ans.get("score", 0.9-i*0.1),
                            "rank": i+1
                        })
                return results
        
        print(f"âš ï¸  API è°ƒç”¨å¤±è´¥: {response.status_code}")
        return []
    except Exception as e:
        print(f"âš ï¸  ç½‘ç»œé”™è¯¯: {e}")
        return []


def extract_evidence_span(snippet: str, query: str) -> dict:
    """ä» snippet ä¸­æå–è¯æ®èŒƒå›´ï¼ˆç®€åŒ–ç‰ˆï¼šæŸ¥æ‰¾æŸ¥è¯¢å…³é”®è¯ï¼‰"""
    if not snippet or not query:
        return {"start": 0, "end": 0}
    
    # ç®€å•å®ç°ï¼šæ‰¾åˆ°æŸ¥è¯¢ä¸­ç¬¬ä¸€ä¸ªè¯åœ¨ snippet ä¸­çš„ä½ç½®
    query_words = query.lower().split()[:3]  # å–å‰3ä¸ªè¯
    for word in query_words:
        if len(word) > 3:  # è·³è¿‡åœç”¨è¯
            idx = snippet.lower().find(word)
            if idx != -1:
                return {"start": max(0, idx-20), "end": min(len(snippet), idx+len(word)+20)}
    
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›å¼€å¤´éƒ¨åˆ†
    return {"start": 0, "end": min(50, len(snippet))}


def calculate_rank_delta(on_results: list, off_results: list) -> dict:
    """è®¡ç®—å…±åŒæ–‡æ¡£çš„æ’åå˜åŒ–"""
    # æ„å»º ID -> rank æ˜ å°„
    on_ranks = {r["id"]: r["rank"] for r in on_results}
    off_ranks = {r["id"]: r["rank"] for r in off_results}
    
    # æ‰¾åˆ°å…±åŒæ–‡æ¡£
    common_ids = set(on_ranks.keys()) & set(off_ranks.keys())
    
    if not common_ids:
        return {"best_rank_delta": 0, "common_docs": 0}
    
    # è®¡ç®—æ¯ä¸ªå…±åŒæ–‡æ¡£çš„ rank_delta = rank_off - rank_on (æ­£æ•°=æ’åæå‡)
    deltas = []
    for doc_id in common_ids:
        delta = off_ranks[doc_id] - on_ranks[doc_id]
        deltas.append(delta)
    
    best_delta = max(deltas) if deltas else 0
    return {"best_rank_delta": best_delta, "common_docs": len(common_ids)}


def create_compare_batch(n: int = 20) -> list:
    """åˆ›å»ºå¯¹æ¯”é›†ï¼šå›ºå®š ON=PageIndex+Reranker, OFF=Baseline"""
    import time
    
    # ä½¿ç”¨çœŸå®æŸ¥è¯¢é›†ï¼ˆä» fiqa_queries.txt æˆ–ç”Ÿæˆï¼‰
    queries = load_fiqa_queries(n)
    
    batch = []
    print(f"ğŸ” ç”Ÿæˆå¯¹æ¯”é›† (n={n})...")
    
    for idx, query in enumerate(queries):
        print(f"  [{idx+1}/{len(queries)}] {query[:60]}...")
        
        # è·å– ON (PageIndex+Reranker) å’Œ OFF (Baseline) çš„ Top-10
        on_results = fetch_real_results(query, "on", top_k=10)
        time.sleep(0.35)
        off_results = fetch_real_results(query, "off", top_k=10)
        time.sleep(0.35)
        
        if not on_results or not off_results:
            print(f"    âš ï¸  è·³è¿‡ï¼ˆAPI å¤±è´¥ï¼‰")
            continue
        
        # è®¡ç®— rank_delta
        rank_info = calculate_rank_delta(on_results, off_results)
        
        # æå– ON Top-1 çš„ evidence_span
        if on_results:
            evidence_span = extract_evidence_span(on_results[0]["snippet"], query)
            on_results[0]["evidence_span"] = evidence_span
        
        # è·å–è§¦å‘åŸå› ï¼ˆä»ç°æœ‰é€»è¾‘ï¼‰
        trigger_reason = get_trigger_reason(query, on_results)
        
        batch.append({
            "id": idx,
            "query": query,
            "on": on_results,
            "off": off_results,
            "best_rank_delta": rank_info["best_rank_delta"],
            "common_docs": rank_info["common_docs"],
            "trigger_reason": trigger_reason
        })
    
    return batch


def load_fiqa_queries(n: int) -> list:
    """åŠ è½½çœŸå® FIQA æŸ¥è¯¢"""
    queries_file = Path(__file__).parent.parent / "data" / "fiqa_queries.txt"
    
    if queries_file.exists():
        with open(queries_file) as f:
            queries = [line.strip() for line in f if line.strip()]
            return random.sample(queries, min(n, len(queries)))
    
    # Fallback: ç”Ÿæˆæ¨¡æ‹ŸæŸ¥è¯¢
    return [
        "What is an ETF?",
        "How to maximize 401k contributions?",
        "Should I refinance my mortgage?",
        "How to improve credit score quickly?",
        "What are common tax deductions?",
        "Best investment strategies for beginners",
        "Difference between Roth and traditional IRA",
        "How does compound interest work?",
        "When should I start saving for retirement?",
        "What is a good debt to income ratio?",
        "How to calculate mortgage payments?",
        "What are index funds?",
        "How to diversify investment portfolio?",
        "What is dollar cost averaging?",
        "Should I pay off debt or invest?",
        "How much emergency fund do I need?",
        "What is the best budgeting method?",
        "How to negotiate salary increase?",
        "What are tax-advantaged accounts?",
        "How to plan for early retirement?",
    ][:n]


def get_trigger_reason(query: str, results: list) -> str:
    """è·å–è§¦å‘åŸå› ï¼ˆé•¿åº¦/å…³é”®è¯/åˆ†æ•£åº¦ï¼‰"""
    reasons = []
    
    # é•¿åº¦æ£€æŸ¥
    if len(query) >= 50:
        reasons.append("len")
    
    # å…³é”®è¯æ£€æŸ¥
    keywords = ["how to", "should i", "best", "calculate", "what is", "when to"]
    if any(kw in query.lower() for kw in keywords):
        reasons.append("kw")
    
    # åˆ†æ•£åº¦æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if results and len(results) >= 3:
        scores = [r.get("score", 0) for r in results[:3]]
        if max(scores) - min(scores) > 0.15:
            reasons.append("dispersion")
    
    return "|".join(reasons) if reasons else "none"


def stratified_sample(results, n=20, four_way=False):
    """åˆ†å±‚é‡‡æ ·ï¼šçŸ­æŸ¥è¯¢ 50% + é•¿æŸ¥è¯¢ 50% æˆ– å››å±‚åˆ†å¸ƒï¼ˆçŸ­/é•¿/è®¡ç®—/ç­–ç•¥ï¼‰"""
    if four_way:
        # å››å±‚åˆ†å±‚ï¼šçŸ­(7) / é•¿(7) / è®¡ç®—(8) / ç­–ç•¥(8)
        # åˆ†ç±»é€»è¾‘ï¼šæŒ‰ query_length å’Œå…³é”®è¯
        compute_kw = ["calculate", "how much", "percent", "rate", "ratio", "cost", "payment"]
        strategy_kw = ["should i", "best", "when to", "how to", "strategy", "plan", "optimize"]
        
        compute_queries = []
        strategy_queries = []
        short_queries = []
        long_queries = []
        
        for r in results:
            q_lower = r["query"].lower()
            q_len = r["query_length"]
            
            # ä¼˜å…ˆæŒ‰å†…å®¹åˆ†ç±»
            if any(kw in q_lower for kw in compute_kw):
                compute_queries.append(r)
            elif any(kw in q_lower for kw in strategy_kw):
                strategy_queries.append(r)
            # å¦åˆ™æŒ‰é•¿åº¦åˆ†ç±»
            elif q_len < 50:
                short_queries.append(r)
            else:
                long_queries.append(r)
        
        # åˆ†é…æ•°é‡ï¼ˆçŸ­/é•¿/è®¡ç®—/ç­–ç•¥â‰ˆ7/7/8/8ï¼‰
        n_short = 7 if n >= 30 else int(n * 0.23)
        n_long = 7 if n >= 30 else int(n * 0.23)
        n_compute = 8 if n >= 30 else int(n * 0.27)
        n_strategy = n - n_short - n_long - n_compute
        
        sampled = []
        sampled.extend(random.sample(short_queries, min(n_short, len(short_queries))))
        sampled.extend(random.sample(long_queries, min(n_long, len(long_queries))))
        sampled.extend(random.sample(compute_queries, min(n_compute, len(compute_queries))))
        sampled.extend(random.sample(strategy_queries, min(n_strategy, len(strategy_queries))))
        
        return sampled
    else:
        # åŸæœ‰ä¸¤å±‚åˆ†å±‚ï¼šçŸ­æŸ¥è¯¢ 50% + é•¿æŸ¥è¯¢ 50%
        sorted_by_length = sorted(results, key=lambda x: x["query_length"])
        median_idx = len(sorted_by_length) // 2
        
        short_queries = sorted_by_length[:median_idx]
        long_queries = sorted_by_length[median_idx:]
        
        n_short = n // 2
        n_long = n - n_short
        
        sampled = []
        if len(short_queries) >= n_short:
            sampled.extend(random.sample(short_queries, n_short))
        else:
            sampled.extend(short_queries)
        
        if len(long_queries) >= n_long:
            sampled.extend(random.sample(long_queries, n_long))
        else:
            sampled.extend(long_queries)
        
        return sampled


def create_batch(results, n=20, strategy="mixed", four_way=False):
    """åˆ›å»ºè¯„æµ‹æ‰¹æ¬¡"""
    import time
    
    # å»é‡
    unique_results = {r["query"]: r for r in results}.values()
    results = list(unique_results)
    target_n = n  # ä¿å­˜ç›®æ ‡å€¼ç”¨äºè¡¥è¶³
    
    # é‡‡æ ·ç­–ç•¥
    sample_n = min(n, len(results))  # é‡‡æ ·æ—¶ä¸è¶…è¿‡å¯ç”¨æ•°
    if strategy == "random":
        sampled = random.sample(results, sample_n)
    elif strategy == "stratified":
        sampled = stratified_sample(results, sample_n, four_way=four_way)
    else:  # mixed: 50% random + 50% stratified
        n_random = sample_n // 2
        n_stratified = sample_n - n_random
        sampled = random.sample(results, n_random)
        remaining = [r for r in results if r not in sampled]
        sampled.extend(stratified_sample(remaining, n_stratified, four_way=four_way))
    
    # è‡ªåŠ¨è¡¥è¶³ï¼šå¦‚æœé‡‡æ ·ä¸è¶³ç›®æ ‡ nï¼Œéšæœºè¡¥å……ï¼ˆå…è®¸é‡å¤ï¼‰
    initial_count = len(sampled)
    if initial_count < target_n:
        need = target_n - initial_count
        sampled.extend(random.choices(results, k=need))
        print(f"[SAMPLE] filled from {initial_count} â†’ {len(sampled)} (auto-padded)")
    
    # æ„å»ºæ‰¹æ¬¡è¾“å‡º
    batch = []
    print("ğŸ” è°ƒç”¨çœŸå® API ç”Ÿæˆ ON/OFF ç»“æœ (beir_fiqa_full_ta)...")
    for idx, item in enumerate(sampled):
        query = item["query"]
        print(f"  [{idx+1}/{len(sampled)}] {query[:50]}...")
        
        # è°ƒç”¨çœŸå® API (with delay to respect rate limit)
        on_results = fetch_real_results(query, "on")
        time.sleep(0.35)  # Respect rate limit (3 req/sec)
        off_results = fetch_real_results(query, "off")
        time.sleep(0.35)  # Respect rate limit
        
        # å¦‚æœ API è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æˆ–è·³è¿‡
        if not on_results or not off_results:
            print(f"    âš ï¸  API å¤±è´¥ï¼Œä½¿ç”¨å†å²æ•°æ®")
            on_results = item.get("on_results", [])[:3]
            off_results = item.get("off_results", [])[:3]
            # å¦‚æœå†å²æ•°æ®ä¹Ÿæ²¡æœ‰ï¼Œç”Ÿæˆå ä½ç¬¦
            if not on_results:
                on_results = [{"title": f"Placeholder {i+1}", "text": f"ON result for: {query[:50]}", "source": "placeholder", "score": 0.7-i*0.1} for i in range(3)]
            if not off_results:
                off_results = [{"title": f"Placeholder {i+1}", "text": f"OFF result for: {query[:50]}", "source": "placeholder", "score": 0.6-i*0.1} for i in range(3)]
        
        batch.append({
            "id": idx,
            "query": query,
            "on": on_results[:3],
            "off": off_results[:3],
            "metadata": {
                "topic": item.get("topic", "unknown"),
                "query_length": item["query_length"]
            }
        })
    
    return batch


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆäººå·¥è¯„æµ‹é‡‡æ ·æ‰¹æ¬¡")
    parser.add_argument("--n", type=int, default=20, help="é‡‡æ ·æ•°é‡ (é»˜è®¤ 20)")
    parser.add_argument("--strategy", choices=["random", "stratified", "mixed"], 
                       default="mixed", help="é‡‡æ ·ç­–ç•¥ (é»˜è®¤ mixed)")
    parser.add_argument("--stratify", action="store_true", 
                       help="ä½¿ç”¨å››å±‚åˆ†å±‚é‡‡æ ·ï¼ˆçŸ­/é•¿/è®¡ç®—/ç­–ç•¥â‰ˆ7/7/8/8ï¼‰")
    parser.add_argument("--label", type=str, default=None, 
                       help="æ‰¹æ¬¡æ ‡ç­¾ï¼ˆå¦‚ 'latest'ï¼‰ï¼Œä¼šåˆ›å»ºå¸¦æ ‡ç­¾çš„ç¬¦å·é“¾æ¥")
    parser.add_argument("--compare", action="store_true",
                       help="å¯¹æ¯”æ¨¡å¼ï¼šå›ºå®š ON=PageIndex+Reranker, OFF=Baselineï¼Œç”Ÿæˆå¯¹æ¯”é›†")
    args = parser.parse_args()
    
    # å¯¹æ¯”æ¨¡å¼
    if args.compare:
        batch = create_compare_batch(n=args.n)
        
        # ä¿å­˜å¯¹æ¯”é›†
        output_filename = f"compare_batch_{args.label}.json" if args.label else "compare_batch_latest.json"
        output_path = Path(__file__).parent.parent / "reports" / output_filename
        output_path.parent.mkdir(exist_ok=True)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        improved_count = sum(1 for item in batch if item.get("best_rank_delta", 0) > 0)
        rank_deltas = [item.get("best_rank_delta", 0) for item in batch]
        median_delta = sorted(rank_deltas)[len(rank_deltas)//2] if rank_deltas else 0
        
        # ç»Ÿè®¡è§¦å‘åŸå› 
        reason_counts = {}
        for item in batch:
            reason = item.get("trigger_reason", "none")
            reason_counts[reason] = reason_counts.get(reason, 0) + 1
        top_reasons = sorted(reason_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        top_reasons_str = ", ".join([f"{r}:{c}" for r, c in top_reasons])
        
        output_data = {
            "batch_id": args.label or "latest",
            "created_at": datetime.now().isoformat(),
            "total": len(batch),
            "improved_count": improved_count,
            "median_rank_delta": median_delta,
            "mode": "compare",
            "items": batch
        }
        
        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å¯¹æ¯”é›†å·²ç”Ÿæˆ: {output_path}")
        print(f"\n[COMPARE] n={len(batch)} | improved={improved_count}/{len(batch)} | median_rank_delta=+{median_delta} | top_reasons={top_reasons_str}")
        print(f"\nğŸ”— æŸ¥çœ‹æŠ¥å‘Š: http://localhost:8080/judge/report")
        return
    
    # åŸæœ‰çš„æ™®é€šæ¨¡å¼
    # åŠ è½½ç»“æœ
    print("ğŸ“‚ åŠ è½½å†å²ç»“æœ...")
    results = load_latest_results()
    print(f"âœ“ åŠ è½½ {len(results)} æ¡ç»“æœ")
    
    # åˆ›å»ºæ‰¹æ¬¡
    four_way = args.stratify
    print(f"ğŸ² é‡‡æ ·ç­–ç•¥: {args.strategy}, ç›®æ ‡æ•°é‡: {args.n}, å››å±‚åˆ†å±‚: {four_way}")
    batch = create_batch(results, n=args.n, strategy=args.strategy, four_way=four_way)
    
    # ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = Path(__file__).parent.parent / "reports" / f"judge_batch_{timestamp}.json"
    output_path.parent.mkdir(exist_ok=True)
    
    output_data = {
        "batch_id": timestamp,
        "created_at": datetime.now().isoformat(),
        "total": len(batch),
        "strategy": args.strategy,
        "four_way": four_way,
        "items": batch
    }
    
    with open(output_path, 'w') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æ‰¹æ¬¡å·²ç”Ÿæˆ: {output_path}")
    print(f"   æ‰¹æ¬¡ID: {timestamp}")
    print(f"   æ ·æœ¬æ•°: {len(batch)}")
    
    # å¦‚æœæŒ‡å®šäº† labelï¼Œåˆ›å»ºç¬¦å·é“¾æ¥æˆ–å‰¯æœ¬
    if args.label:
        label_path = Path(__file__).parent.parent / "reports" / f"judge_batch_{args.label}.json"
        import shutil
        shutil.copy(output_path, label_path)
        print(f"   æ ‡ç­¾: {args.label} -> {label_path.name}")
    
    print(f"\nğŸ”— è®¿é—®æ ‡æ³¨é¡µ: http://localhost:8080/judge?batch={args.label or timestamp}")


if __name__ == "__main__":
    main()


