PROJECT ?= searchforge
COMPOSE = docker compose --env-file .env.current -p $(PROJECT)

SSH_HOST ?= andy-wsl
REMOTE ?= $(SSH_HOST)
RDIR=~/searchforge

# Helper to detect current target
TARGET ?= $(shell grep -E '^SEARCHFORGE_TARGET=' .env.current | cut -d= -f2 2>/dev/null || echo local)

## ===== Utilities =====

define ensure_tool
	@command -v $(1) >/dev/null 2>&1 || { echo "âŒ Missing dependency: $(1). Please install it."; exit 1; }
endef

.PHONY: help up down restart rebuild logs ps health prune-safe df tunnel-dozzle open-portainer sync whoami gpu-smoke compose-config update-hosts migrate-qdrant cutover-remote baseline-save baseline-save-local baseline-save-remote ui rebuild-api rebuild-api-cpu up-gpu down-gpu export-reqs lint-no-poetry cleanup-audit cleanup-apply cleanup-restore cleanup-history create-clean-repo sync-experiments verify-experiments smoke-experiment runner-check fiqa-50k-stage-b smoke-fast

# Default target: show help
.DEFAULT_GOAL := help

help: ## æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨å‘½ä»¤ï¼ˆé»˜è®¤å‘½ä»¤ï¼‰
	@echo "=================================================="
	@echo "  SearchForge Makefile å‘½ä»¤å¸®åŠ©"
	@echo "=================================================="
	@echo ""
	@echo "ğŸ“‹ ç¯å¢ƒåˆ‡æ¢ (Environment Switching)"
	@echo "  make whoami              - æŸ¥çœ‹å½“å‰ç›®æ ‡ç¯å¢ƒ"
	@echo "  make update-hosts        - åˆ·æ–°è¿œç¨‹ä¸»æœºåæ˜ å°„ï¼ˆéœ€è¦ sudoï¼‰"
	@echo "  make cutover-remote       - åˆ‡æ¢åˆ°è¿œç¨‹ç¯å¢ƒï¼ˆå¸¦ SLA æ£€æŸ¥ï¼‰"
	@echo "  make compose-config       - æŸ¥çœ‹å½“å‰æœåŠ¡ç«¯ç‚¹é…ç½®"
	@echo ""
	@echo "ğŸ“Š åŸºçº¿ç®¡ç† (Baseline Management)"
	@echo "  make baseline-save-local  - åˆ›å»ºæœ¬åœ°ç¯å¢ƒæ€§èƒ½åŸºçº¿"
	@echo "  make baseline-save-remote - åˆ›å»ºè¿œç¨‹ç¯å¢ƒæ€§èƒ½åŸºçº¿"
	@echo "  make baseline-save       - æ ¹æ®å½“å‰ç›®æ ‡è‡ªåŠ¨åˆ›å»ºåŸºçº¿"
	@echo ""
	@echo "ğŸ”„ è¿œç¨‹æœåŠ¡ç®¡ç† (Remote Service Management)"
	@echo "  make up                  - å¯åŠ¨è¿œç¨‹æœåŠ¡"
	@echo "  make down                - åœæ­¢è¿œç¨‹æœåŠ¡"
	@echo "  make restart             - é‡å¯è¿œç¨‹æœåŠ¡"
	@echo "  make health              - æ£€æŸ¥è¿œç¨‹æœåŠ¡å¥åº·çŠ¶æ€"
	@echo "  make logs                - æŸ¥çœ‹è¿œç¨‹æœåŠ¡æ—¥å¿—"
	@echo "  make ps                  - æŸ¥çœ‹è¿œç¨‹å®¹å™¨çŠ¶æ€"
	@echo ""
	@echo "ğŸ§¹ æ¸…ç†å’Œç»´æŠ¤ (Cleanup & Maintenance)"
	@echo "  make prune-safe          - å®‰å…¨æ¸…ç† Dockerï¼ˆä¿ç•™æ•°æ®å·ï¼‰"
	@echo "  make df                  - æŸ¥çœ‹ Docker ç£ç›˜ä½¿ç”¨æƒ…å†µ"
	@echo "  make cleanup-audit       - å®¡è®¡å¯æ¸…ç†çš„æ–‡ä»¶ï¼ˆdry-runï¼‰"
	@echo "  make cleanup-apply       - å½’æ¡£æœªä½¿ç”¨çš„è„šæœ¬/æµ‹è¯•/æ–‡æ¡£"
	@echo "  make cleanup-restore     - æ¢å¤å½’æ¡£çš„æ–‡ä»¶"
	@echo ""
	@echo "ğŸ”¬ è¯„æµ‹å’Œé‡‘æ ‡å·¥å…· (Evaluation & Gold Standard)"
	@echo "  make eval-qrels          - æ£€æŸ¥ qrels è¦†ç›–ç‡ (10k/50k)"
	@echo "  make eval-consistency    - æ£€æŸ¥æ•°æ®é›†/é›†åˆ/å­—æ®µä¸€è‡´æ€§"
	@echo "  make eval-recall         - è®¡ç®—å»é‡åçš„ Recall@K"
	@echo "  make gold-prepare        - å‡†å¤‡è–„é‡‘æ ‡å€™é€‰ CSV"
	@echo "  make gold-finalize       - ä»æ ‡æ³¨ç”Ÿæˆ qrels_gold.tsv"
	@echo "  make cleanup-history     - æ¸…ç† Git å†å²ä¸­çš„å¤§æ–‡ä»¶ï¼ˆå±é™©æ“ä½œï¼‰"
	@echo ""
	@echo "ğŸ”— éš§é“å’Œè®¿é—® (Tunnels & Access)"
	@echo "  make tunnel-dozzle       - åˆ›å»º Dozzle æ—¥å¿—æŸ¥çœ‹éš§é“ï¼ˆCtrl-C å…³é—­ï¼‰"
	@echo "  make open-portainer      - æ‰“å¼€ Portainer ç®¡ç†ç•Œé¢"
	@echo ""
	@echo "ğŸ› ï¸  å…¶ä»–å·¥å…· (Other Tools)"
	@echo "  make sync                - åŒæ­¥æ–‡ä»¶åˆ°è¿œç¨‹"
	@echo "  make gpu-smoke           - GPU æµ‹è¯•"
	@echo "  make migrate-qdrant      - è¿ç§» Qdrant æ•°æ®åˆ°è¿œç¨‹"
	@echo "  make ui                  - å¯åŠ¨å‰ç«¯ UI Dashboard"
	@echo "  make rebuild-api         - é‡å»ºå¹¶é‡å¯ rag-api æœåŠ¡"
	@echo "  make rebuild-api-cpu      - é‡å»º CPU-only rag-api å¹¶éªŒè¯æ—  CUDA"
	@echo "  make win-fw-allow-8000    - æ‰“å° Windows é˜²ç«å¢™æ”¾è¡Œ 8000 (Tailscale æ®µ)"
	@echo "  make net-verify           - ç«¯å£ä¸å¥åº·æ£€æŸ¥ (å®¹å™¨å†…/å¤–)"
	@echo "  make up-gpu              - å¯åŠ¨ GPU worker æœåŠ¡ï¼ˆå¯é€‰ï¼‰"
	@echo "  make down-gpu            - åœæ­¢ GPU worker æœåŠ¡"
	@echo ""
	@echo "ğŸ§ª å®éªŒç®¡ç† (Experiment Management)"
	@echo "  make sync-experiments    - åŒæ­¥å®éªŒè„šæœ¬åˆ°è¿œç¨‹æœåŠ¡å™¨"
	@echo "  make verify-experiments  - éªŒè¯è¿œç¨‹å®éªŒæ–‡ä»¶æ˜¯å¦å­˜åœ¨"
	@echo "  make runner-check        - æ£€æŸ¥ runner è‡ªæ£€ç«¯ç‚¹"
	@echo "  make smoke-experiment    - è¿è¡Œæœ€å°å®éªŒï¼ˆsample=5ï¼‰"
	@echo ""
	@echo "ğŸ”’ Phase A: Baseline + Presets + Guards"
	@echo "  make guard-no-cuda       - æ£€æŸ¥æœ¬åœ°ç¯å¢ƒæ—  CUDA åŒ…"
	@echo "  make embed-doctor       - æ£€æŸ¥ embedding æ¨¡å‹é…ç½®"
	@echo "  make baseline-run       - æäº¤ baseline å®éªŒ"
	@echo "  make baseline-poll       - è½®è¯¢ baseline ä»»åŠ¡çŠ¶æ€"
	@echo "  make baseline-artifacts  - ä¸‹è½½ baseline ç»“æœ"
	@echo ""
	@echo "ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹ï¼š"
	@echo "  make help                - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯"
	@echo "  make cutover-remote       - åˆ‡æ¢åˆ°è¿œç¨‹ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰"
	@echo "  N=150 C=10 WARMUP=10 TIMEOUT=3 make cutover-remote  - è‡ªå®šä¹‰å‚æ•°åˆ‡æ¢"
	@echo ""
	@echo "=================================================="

sync:
	@rsync -avzP mini-d-files/ $(REMOTE):$(RDIR)/

ssh-ok:
	@ssh andy-wsl 'echo ok'

up:
	@ssh $(SSH_HOST) 'cd $(RDIR) && cp -n .env.sample .env || true && docker compose up -d --build'
	@$(MAKE) health

down:
	@ssh $(SSH_HOST) 'cd $(RDIR) && docker compose down'

restart:  ## Restart backend service on remote
	@ssh $(SSH_HOST) 'cd $(RDIR) && docker compose restart rag-api || docker compose up -d rag-api'
	@sleep 5
	@$(MAKE) health

rebuild: rebuild-api

logs:
	@ssh $(SSH_HOST) 'cd $(RDIR) && docker compose logs -f --tail=200 api'

ps:
	@ssh $(SSH_HOST) 'docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"'

health:
	@ssh $(SSH_HOST) 'curl -fsS http://localhost:8000/health && echo'

prune-safe:
	@ssh $(SSH_HOST) 'docker system prune -af' # ä¸å¸¦ --volumesï¼Œé¿å…è¯¯åˆ æ•°æ®å·

df:
	@ssh $(SSH_HOST) 'docker system df -v'

# æœ¬æœºå¼€ä¸€ä¸ªéš§é“ï¼šè®¿é—® http://localhost:9999 æŸ¥çœ‹ Dozzle
tunnel-dozzle:
	@echo "Press Ctrl-C to close the tunnel."
	@ssh -N -L 9999:127.0.0.1:9999 $(REMOTE)

# å¿«æ·æ‰“å¼€ Portainerï¼ˆæŠŠ 100.x.x.x æ¢æˆä½ çš„ Tailscale IPï¼‰
open-portainer:
	@open https://100.67.88.114:9443

whoami:
	@bash tools/switch/print_target.sh

gpu-smoke:
	@$(COMPOSE) run --rm --gpus all gpu-smoke nvidia-smi -L

compose-config:
	@echo "Service endpoints from .env.current:"
	@grep -E '^RAG_API_BASE=|^QDRANT_URL=' .env.current 2>/dev/null || echo "âš ï¸  RAG_API_BASE or QDRANT_URL not found in .env.current"

update-hosts:
	@bash tools/switch/update_hosts.sh

migrate-qdrant:
	@bash tools/switch/migrate_qdrant_to_remote.sh

cutover-remote:
	@pip install -q aiohttp || true
	@mkdir -p artifacts/sla/manifests
	@bash tools/switch/cutover_remote.sh

baseline-save-local:
	@mkdir -p artifacts/sla
	@echo "Running baseline smoke test against local RAG API..."
	@RAG_API_BASE=http://localhost:8000 python3 tools/switch/smoke.py --n 200 --concurrency 10 --warmup 20 --timeout 3 --base http://localhost:8000 > artifacts/sla/baseline.local.json
	@echo "âœ… Baseline saved to artifacts/sla/baseline.local.json"

baseline-save-remote:
	@mkdir -p artifacts/sla
	@echo "Running baseline smoke test against remote RAG API..."
	@RAG_API_BASE=http://andy-wsl:8000 python3 tools/switch/smoke.py --n 200 --concurrency 10 --warmup 20 --timeout 3 --base http://andy-wsl:8000 > artifacts/sla/baseline.remote.json
	@echo "âœ… Baseline saved to artifacts/sla/baseline.remote.json"

baseline-save:
	@if [[ "$(TARGET)" == remote* ]]; then \
		$(MAKE) baseline-save-remote; \
	else \
		$(MAKE) baseline-save-local; \
	fi

ui:
	@echo "ğŸš€ Starting Vite dev server (ui)..."
	@echo "ğŸ“ API base: http://andy-wsl:8000"
	@echo "ğŸŒ Dev server: http://localhost:5173"
	@cd ui && \
		if [ ! -d "node_modules" ]; then \
			echo "ğŸ“¦ Installing dependencies..."; \
			npm install; \
		fi && \
		npm run dev -- --port 5173 --open --host

rebuild-api: export-reqs
	@echo "ğŸ”¨ Rebuilding rag-api service..."
	@ssh $(SSH_HOST) 'cd $(RDIR) && docker compose build rag-api && docker compose up -d rag-api'
	@echo "â³ Waiting for service to be ready..."
	@sleep 5
	@$(MAKE) health

rebuild-api-cpu: ## é‡å»º CPU-only rag-api å¹¶éªŒè¯æ—  CUDA åŒ…
	@echo "ğŸ”¨ Rebuilding CPU-only rag-api service..."
	@ssh $(SSH_HOST) 'cd $(RDIR) && docker compose build --no-cache rag-api && docker compose up -d rag-api'
	@echo "â³ Waiting for service to be ready..."
	@for i in $$(seq 1 30); do \
		echo "â³ waiting ($$i/30)..."; \
		sleep 1; \
		curl -fsS http://andy-wsl:8000/health >/dev/null 2>&1 && break || true; \
	done
	@$(MAKE) net-verify
	@$(MAKE) guard-no-cuda
	@$(MAKE) embed-doctor
	@echo "âœ… CPU-only rebuild complete and verified"

guard-no-cuda: ## æ£€æŸ¥å®¹å™¨ä¸­æ˜¯å¦åŒ…å« CUDA åŒ…
	@echo "ğŸ” Checking for CUDA packages in container..."
	@docker compose exec -T rag-api python3 tools/guards/check_no_cuda_local.py || (echo "âš ï¸  CUDA packages detected (non-fatal for CPU-only SBERT)" && exit 0)
	@echo "âœ… No CUDA packages found"

embed-doctor: ## æ£€æŸ¥ embedding æ¨¡å‹é…ç½®
	@echo "ğŸ” Checking embedding model configuration..."
	@API_BASE=$$(curl -fsS http://127.0.0.1:8000/health >/dev/null 2>&1 && echo "http://127.0.0.1:8000" || echo "http://localhost:8000"); \
	curl -fsS $$API_BASE/api/health/embeddings | python3 -c "import sys, json; d=json.load(sys.stdin); print(json.dumps(d, indent=2))" || (echo "âŒ Embedding model check failed"; exit 1)
	@echo "âœ… Embedding model consistency check passed"

win-fw-allow-8000: ## æ‰“å° Windows é˜²ç«å¢™æ”¾è¡Œ 8000 çš„ PowerShell å‘½ä»¤
	@echo 'ä»¥ç®¡ç†å‘˜ PowerShell æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š'
	@echo 'New-NetFirewallRule -DisplayName "SearchForge rag-api 8000 Tailscale" -Direction Inbound -Protocol TCP -LocalPort 8000 -RemoteAddress 100.64.0.0/10 -Action Allow'

net-verify: ## éªŒè¯ rag-api ç«¯å£ç»‘å®šä¸å¥åº·æ¥å£
	@echo "ğŸ” Checking container port bindings (remote)..."
	@ssh $(SSH_HOST) 'docker ps --format "table {{.Names}}\t{{.Ports}}" | grep rag-api || true'
	@echo "ğŸ” Curl health from inside container (127.0.0.1:${MAIN_PORT})..."
	@ssh $(SSH_HOST) 'cd $(RDIR) && docker compose exec -T rag-api sh -lc "curl -fsS http://127.0.0.1:${MAIN_PORT}/health || curl -fsS http://127.0.0.1:8000/health"'
	@echo "ğŸ” Curl health from Mac (andy-wsl:8000)..."
	@curl -fsS http://andy-wsl:8000/health || (sleep 2; curl -fsS http://andy-wsl:8000/health)
	@echo "âœ… Network verification done"

up-gpu:
	@echo "ğŸš€ Starting GPU worker service..."
	@ssh $(SSH_HOST) 'cd $(RDIR) && docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d gpu-worker'
	@echo "âœ… GPU worker started"

down-gpu:
	@echo "ğŸ›‘ Stopping GPU worker service..."
	@ssh $(SSH_HOST) 'cd $(RDIR) && docker compose -f docker-compose.yml -f docker-compose.gpu.yml down gpu-worker'
	@echo "âœ… GPU worker stopped"

# Repository cleanup targets (safe, reversible archiving)
cleanup-audit: ## å®¡è®¡å¯æ¸…ç†çš„æ–‡ä»¶ï¼ˆdry-runï¼Œç”Ÿæˆå€™é€‰åˆ—è¡¨ï¼‰
	@bash tools/cleanup/audit.sh

cleanup-apply: ## å½’æ¡£æœªä½¿ç”¨çš„è„šæœ¬/æµ‹è¯•/æ–‡æ¡£åˆ° archive/
	@bash tools/cleanup/apply.sh

cleanup-restore: ## æ¢å¤å½’æ¡£çš„æ–‡ä»¶åˆ°åŸå§‹ä½ç½®
	@bash tools/cleanup/restore.sh

cleanup-history: ## æ¸…ç† Git å†å²ä¸­çš„å¤§æ–‡ä»¶ï¼ˆéœ€è¦ I_KNOW_WHAT_IM_DOING=1ï¼‰
	@bash tools/cleanup/slim_history.sh

create-clean-repo: ## åˆ›å»ºå¹²å‡€çš„ä»“åº“å¿«ç…§å¹¶åˆ‡æ¢åˆ°æ–°è¿œç¨‹ï¼ˆéœ€è¦ NEW_REPO_URL=<url>ï¼‰
	@bash tools/cleanup/create_clean_repo.sh

export-reqs: ## Export Poetry dependencies to requirements.txt (dev-only)
	@if [ -f "pyproject.toml" ] && command -v poetry >/dev/null 2>&1; then \
		echo "ğŸ“¦ Exporting Poetry dependencies to requirements.txt..."; \
		poetry export -f requirements.txt --without-hashes -o services/rag_api/requirements.txt || true; \
		echo "âœ… Exported to services/rag_api/requirements.txt"; \
	else \
		echo "âš ï¸  Poetry not available or pyproject.toml not found, skipping export"; \
	fi

lint-no-poetry: ## Check that no 'poetry run' appears in runtime paths
	@echo "ğŸ” Checking for 'poetry run' in runtime paths..."
	@if git grep -nE 'poetry\s+run' -- 'services/**' 'tools/**' 'Makefile' '**/Dockerfile' 'docker-compose*.yml' >/dev/null 2>&1; then \
		echo "âŒ ERROR: 'poetry run' found in runtime paths:"; \
		git grep -nE 'poetry\s+run' -- 'services/**' 'tools/**' 'Makefile' '**/Dockerfile' 'docker-compose*.yml'; \
		exit 1; \
	else \
		echo "âœ… No 'poetry run' found in runtime paths"; \
	fi

# Experiment management targets
sync-experiments: ## Sync experiments directory to remote server
	@bash tools/experiments/sync_experiments.sh

verify-experiments: ## Verify experiment files exist on remote server
	@bash tools/experiments/verify_remote.sh

runner-check: ## Check runner self-check endpoint
	@echo "ğŸ” Checking runner status..."
	@curl -fsS http://andy-wsl:8000/api/experiment/runner_check | python3 -m json.tool

smoke-experiment: ## Run minimal experiment (sample=5) to verify setup
	@echo "ğŸ§ª Running smoke test experiment (sample=5)..."
	@curl -sX POST http://andy-wsl:8000/api/experiment/run \
		-H 'content-type: application/json' \
		-d '{"preset_name":"fiqa_baseline_10k"}' | python3 -m json.tool > /tmp/smoke_job.json
	@JOB_ID=$$(python3 -c "import json; print(json.load(open('/tmp/smoke_job.json'))['job_id'])") && \
		echo "" && \
		echo "âœ… Job submitted: $$JOB_ID" && \
		echo "ğŸ“Š Check status: curl http://andy-wsl:8000/api/experiment/status/$$JOB_ID" && \
		echo "ğŸ“œ Check logs: curl http://andy-wsl:8000/api/experiment/logs/$$JOB_ID"

smoke-fast: ## Run quick backend smoke test against local endpoints
	@bash scripts/quick_backend_smoke.sh

smoke-review: ## Run steward review/apply smoke check (requires JOB_ID=<job>)
	@bash scripts/smoke_review_llm.sh

smoke-metrics: ## Run metrics smoke check (ensures p95/log summary populated)
	@bash scripts/smoke_metrics.sh

fiqa-50k-stage-b: ## FiQA 50k Stage-B: Full Evaluation of Winners
	$(call ensure_tool,poetry)
	@echo "ğŸ” FiQA 50k Stage-B: Full Evaluation of Winners"
	@echo "Step 1/2: Running full evaluation..."
	@poetry run python experiments/run_50k_grid.py \
		--suite experiments/suite_50k_stage_b.yaml \
		--winners reports/fiqa_50k/winners.json \
		--stage b
	@echo ""
	@echo "Step 2/2: Generating plots..."
	@poetry run python experiments/plot_50k.py --in reports/fiqa_50k/stage_b --out reports/fiqa_50k/stage_b
	@echo "âœ… FiQA 50k Stage-B complete! Check reports/fiqa_50k/stage_b/"

# ========================================
# Phase A: Baseline + Presets + Guards
# ========================================

baseline-run: ## Submit baseline experiment (FIQA Fast - Baseline 50k)
	@echo "Submitting baseline (FIQA Fast - Baseline 50k)..."
	@API_BASE=$$(curl -fsS http://127.0.0.1:8000/health >/dev/null 2>&1 && echo "http://127.0.0.1:8000" || echo "http://localhost:8000"); \
	curl -fsS -H "content-type: application/json" \
	  -d '{"sample":200,"repeats":1,"fast_mode":false, "dataset_name":"fiqa_50k_v1","qrels_name":"fiqa_qrels_50k_v1"}' \
	  $$API_BASE/api/experiment/run | tee /tmp/baseline_run.json
	@echo "âœ… Baseline job submitted"

baseline-poll: ## Poll baseline job status until completion
	@JOB=$$(python3 -c "import json;print(json.load(open('/tmp/baseline_run.json'))['job_id'])"); \
	echo "JOB=$$JOB"; \
	API_BASE=$$(curl -fsS http://127.0.0.1:8000/health >/dev/null 2>&1 && echo "http://127.0.0.1:8000" || echo "http://localhost:8000"); \
	for i in $$(seq 1 120); do \
	  R=$$(curl -fsS $$API_BASE/api/experiment/status/$$JOB 2>/dev/null); \
	  echo "Status check $$i:"; \
	  echo "$$R" | python3 -c "import sys, json; print(json.dumps(json.load(sys.stdin), indent=2))" 2>/dev/null || echo "$$R"; \
	  S=$$(echo "$$R" | python3 -c "import sys, json; d=json.load(sys.stdin); print((d.get('job') or {}).get('status', 'unknown'))" 2>/dev/null || echo "unknown"); \
	  [ "$$S" = "SUCCEEDED" ] && break; \
	  [ "$$S" = "FAILED" ] && break; \
	  sleep 2; \
	done; \
	echo $$JOB > /tmp/baseline_job_id; \
	echo "âœ… Job $$JOB finished with status $$S"

baseline-artifacts: ## Download artifacts for baseline job
	@JOB=$$(cat /tmp/baseline_job_id); \
	echo "Downloading artifacts for $$JOB ..."; \
	mkdir -p artifacts/$$JOB && \
	curl -fsS "http://andy-wsl:8000/api/experiment/logs/$$JOB?tail=5000" -o artifacts/$$JOB/logs.txt && \
	echo "âœ… Artifacts saved to artifacts/$$JOB/"

# ========================================
# Evaluation & Gold Standard Tools
# ========================================

eval-qrels: ## Check qrels coverage for 10k and 50k datasets
	@echo "ğŸ” Checking qrels coverage..."
	@python3 tools/eval/qrels_doctor.py \
	  --qrels experiments/data/fiqa/fiqa_qrels_10k_v1.tsv \
	  --collection fiqa_10k_v1 \
	  --out reports/qrels_coverage_10k.json || (echo "âŒ Qrels 10k coverage check failed"; exit 1)
	@python3 tools/eval/qrels_doctor.py \
	  --qrels experiments/data/fiqa/fiqa_qrels_50k_v1.tsv \
	  --collection fiqa_50k_v1 \
	  --out reports/qrels_coverage_50k.json || (echo "âŒ Qrels 50k coverage check failed"; exit 1)
	@echo "âœ… Qrels coverage check complete"

eval-consistency: ## Check dataset/collection/field and embed consistency
	@echo "ğŸ” Checking consistency..."
	@python3 tools/eval/consistency_check.py \
	  --dataset-name fiqa_50k_v1 \
	  --fields title,text \
	  --out reports/consistency.json || (echo "âŒ Consistency check failed"; exit 1)
	@echo "âœ… Consistency check complete"

eval-recall: ## Compute de-duplicated Recall@K for latest run
	@echo "ğŸ” Computing Recall@K..."
	@if [ -z "$$RUN_FILE" ]; then \
	  echo "ERROR: Set RUN_FILE environment variable"; \
	  exit 1; \
	fi
	@python3 tools/eval/recall_eval_dedup.py \
	  --run $$RUN_FILE \
	  --qrels experiments/data/fiqa/fiqa_qrels_50k_v1.tsv \
	  --k 10 \
	  --out reports/recall_at_k.json
	@echo "âœ… Recall evaluation complete"

gold-prepare: ## Prepare gold standard candidate CSV
	@echo "ğŸ“ Preparing gold standard candidates..."
	@if [ -z "$$QUERIES_FILE" ] || [ -z "$$RUNS_FILE" ]; then \
	  echo "ERROR: Set QUERIES_FILE and RUNS_FILE environment variables"; \
	  exit 1; \
	fi
	@python3 tools/eval/gold_prepare.py \
	  --queries $$QUERIES_FILE \
	  --runs $$RUNS_FILE \
	  --bm25-runs $$BM25_RUNS_FILE \
	  --per-query 20 \
	  --collection fiqa_50k_v1 \
	  --out reports/gold_candidates.csv
	@echo "âœ… Gold candidates prepared. Open reports/gold_candidates.csv to label."

gold-finalize: ## Generate qrels_gold.tsv from labeled CSV
	@echo "ğŸ“ Generating qrels_gold.tsv..."
	@if [ -z "$$LABELS_FILE" ]; then \
	  echo "ERROR: Set LABELS_FILE environment variable"; \
	  exit 1; \
	fi
	@python3 tools/eval/gold_finalize.py \
	  --labels $$LABELS_FILE \
	  --out reports/qrels_gold.tsv
	@echo "âœ… Qrels gold standard generated: reports/qrels_gold.tsv"

gold-gate: ## Quality gate: compare Recall@10 against baseline
	@echo "ğŸšª Running gold standard quality gate..."
	@if [ ! -f "reports/qrels_gold.tsv" ]; then \
	  echo "ERROR: reports/qrels_gold.tsv not found. Run 'make gold-finalize' first."; \
	  exit 1; \
	fi
	@if [ -z "$$RUN_FILE" ]; then \
	  echo "WARNING: RUN_FILE not set. Using latest run from reports/..."; \
	  RUN_FILE=$$(ls -t reports/*_runs.jsonl 2>/dev/null | head -1); \
	  if [ -z "$$RUN_FILE" ]; then \
	    echo "ERROR: No run file found. Set RUN_FILE environment variable."; \
	    exit 1; \
	  fi; \
	fi
	@echo "Using run file: $$RUN_FILE"
	@python3 tools/eval/recall_eval_dedup.py \
	  --run $$RUN_FILE \
	  --qrels reports/qrels_gold.tsv \
	  --k 10 \
	  --out reports/gold_recall_at_k.json || (echo "âŒ Recall evaluation failed"; exit 2)
	@GOLD_RECALL=$$(python3 -c "import json; print(json.load(open('reports/gold_recall_at_k.json'))['metrics']['mean_recall_at_k'])") && \
	BASELINE_RECALL=0.9995 && \
	DIFF=$$(python3 -c "print($$BASELINE_RECALL - $$GOLD_RECALL)") && \
	if [ $$(echo "$$DIFF > 0.01" | bc -l 2>/dev/null || python3 -c "print(1 if $$DIFF > 0.01 else 0)") -eq 1 ]; then \
	  echo "âš ï¸  Gold Recall@10 ($$GOLD_RECALL) is >1% below baseline ($$BASELINE_RECALL)"; \
	  echo "   Difference: $$DIFF"; \
	  exit 2; \
	else \
	  echo "âœ… Gold Recall@10 ($$GOLD_RECALL) within 1% of baseline ($$BASELINE_RECALL)"; \
	fi

gold-update-presets: ## Update presets with gold qrels mappings
	@echo "ğŸ“ Updating presets with gold qrels..."
	@if [ ! -f "reports/qrels_gold.tsv" ]; then \
	  echo "WARNING: reports/qrels_gold.tsv not found. Presets will still be updated."; \
	fi
	@DATASET_NAME=$${DATASET_NAME:-fiqa_50k_v1}; \
	QRELS_NAME=$${QRELS_NAME:-fiqa_qrels_50k_v1_gold}; \
	COLLECTION=$${COLLECTION:-fiqa_50k_v1}; \
	echo "Using DATASET_NAME=$$DATASET_NAME, QRELS_NAME=$$QRELS_NAME, COLLECTION=$$COLLECTION"; \
	python3 tools/eval/update_presets_gold.py \
	  --presets-file configs/presets_v10.json \
	  --gold-qrels-name $$QRELS_NAME \
	  --dataset-name $$DATASET_NAME \
	  --collection $$COLLECTION || (echo "âŒ Failed to update presets"; exit 1)
	@echo "âœ… Presets updated. Gold presets available with qrels_name: $${QRELS_NAME:-fiqa_qrels_50k_v1_gold}"
